{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce using SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# needed to get this to work \n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [SPARK](#SPARK)\n",
    "    * Installing Spark locally\n",
    "* [Spark Context](#Spark-Context)\n",
    "    * [Create A RDD](#Create-A-RDD)\n",
    "    * [Call `collect` on an RDD: Lazy Spark](#Call-collect-on-an-RDD:-Lazy-Spark)\n",
    "    * [Operations on RDDs](#Operations-on-RDDs)\n",
    "    * [Word Examples](#Word-Examples)\n",
    "    * [Key Value Pairs](#Key-Value-Pairs)\n",
    "    * [word count 1](#word-count-1)\n",
    "    * [word count 2:  `reduceByKey()`](#word-count-2:--reduceByKey%28%29)\n",
    "    * [Nested Syntax](#Nested-Syntax)\n",
    "    * [Using Cache](#Using-Cache)\n",
    "    * [Fun with words](#Fun-with-words)\n",
    "    * [DataFrames](#DataFrames)\n",
    "    * [Machine Learning](#Machine-Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With shameless stealing of some code and text from:\n",
    "\n",
    "- https://github.com/tdhopper/rta-pyspark-presentation/blob/master/slides.ipynb\n",
    "- Databricks and Berkeley Spark MOOC: https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\n",
    "\n",
    "which you should go check out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Context\n",
    "\n",
    "You can also use it directly from the notebook interface on the mac if you installed `apache-spark` using `brew` and also installed `findspark` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.14:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).map(lambda x: x**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141352\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create A RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "print (type(wordsRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Call `collect` on an RDD: Lazy Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant', 'rat', 'rat', 'cat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Spark Programming Guide:\n",
    "\n",
    ">RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n"
     ]
    }
   ],
   "source": [
    "def makePlural(word):\n",
    "    return word + 's'\n",
    "\n",
    "print(makePlural('cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform one RDD into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "['cats', 'elephants']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD = wordsRDD.map(makePlural)\n",
    "print(pluralRDD.first())\n",
    "print(pluralRDD.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pluralRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'elephants', 'rats', 'rats', 'cats']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda w: (w, 1))\n",
    "print(wordPairs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WORD COUNT!\n",
    "\n",
    "This little exercise shows how to use mapreduce to calculate the counts of individual words in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('elephant', 1), ('rat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda w: (w, 1))\n",
    "                       .reduceByKey(lambda x,y: x+y)\n",
    "                       .collect())\n",
    "print(wordCountsCollected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Tons of shuffling](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[26] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "print(wordsRDD.map(lambda w: (w, 1)).reduceByKey(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[27] at parallelize at PythonRDD.scala:175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "print(wordsRDD)\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, every operation is run from the start. This may be inefficient in many cases. So when appropriate, we may want to cache the result the first time an operation is run on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is rerun from the start\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[27] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default storage level (MEMORY_ONLY)\n",
    "wordsRDD.cache()#nothing done this is still lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parallelize is rerun and cached because we told it to cache\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this `sc.parallelize` is not rerun in this case\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is this useful: it is when you have branching parts or loops, so that you dont do things again and again. Spark, being \"lazy\" will rerun the chain again. So `cache` or `persist` serves as a checkpoint, breaking the RDD chain or the *lineage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 'mammal',\n",
       " 'elephant': 'mammal',\n",
       " 'heron': 'bird',\n",
       " 'owl': 'bird',\n",
       " 'rat': 'mammal'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birdsList=['heron','owl']\n",
    "animList=wordsList+birdsList\n",
    "animaldict={}\n",
    "for e in wordsList:\n",
    "    animaldict[e]='mammal'\n",
    "for e in birdsList:\n",
    "    animaldict[e]='bird'\n",
    "animaldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2\n"
     ]
    }
   ],
   "source": [
    "animsrdd = sc.parallelize(animList, 4)\n",
    "animsrdd.cache()\n",
    "#below runs the whole chain but causes cache to be populated\n",
    "mammalcount=animsrdd.filter(lambda w: animaldict[w]=='mammal').count()\n",
    "#now only the filter is carried out\n",
    "birdcount=animsrdd.filter(lambda w: animaldict[w]=='bird').count()\n",
    "print(mammalcount, birdcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: Fun with MapReduce\n",
    "\n",
    "Read http://spark.apache.org/docs/latest/programming-guide.html for some useful background and then try out the following exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `./sparklect/english.stop.txt` contains a list of English stopwords, while the file `./sparklect/shakes/juliuscaesar.txt` contains the entire text of Shakespeare's 'Julius Caesar'.\n",
    "\n",
    "* Load all of the stopwords into a Python list\n",
    "* Load the text of Julius Caesar into an RDD using the `sparkcontext.textfile()` method. Call it `juliusrdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your turn\n",
    "stopwords = pd.read_csv('./sparklect/english.stop.txt')\n",
    "#sc.stop()\n",
    "#sc = pyspark.SparkContext()\n",
    "juliusrdd = sc.textFile('./sparklect/shakes/juliuscaesar.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words does Julius Caesar have? *Hint: use `flatMap()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your turn\n",
    "#counts = juliusrdd.flatMap(lambda l: l.split(' ')).map(lambda w: (w,1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "\n",
    "wordcount = juliusrdd.flatMap(lambda l: l.split(' ')).map(lambda w: (w,1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 20 words of Julius Caesar as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 12571)\n",
      "('him-let', 1)\n",
      "('was', 57)\n",
      "('So,', 2)\n",
      "('Citizens,', 1)\n",
      "(\"'tis\", 13)\n",
      "('matter;', 1)\n",
      "('head', 2)\n",
      "('dark', 1)\n",
      "('avoided', 1)\n",
      "('wine!', 1)\n",
      "('Choose', 1)\n",
      "(\"fear'd\", 2)\n",
      "('Freedom!', 1)\n",
      "('choked', 2)\n",
      "('According', 2)\n",
      "('goes;', 1)\n",
      "('USE', 5)\n",
      "('benefit', 1)\n",
      "(\"bay'd,\", 1)\n"
     ]
    }
   ],
   "source": [
    "# your turn\n",
    "for i in range(20):\n",
    "    print(wordcount[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 20 words of Julius Caesar, **after removing all the stopwords**. *Hint: use `filter()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               a\n",
      "0            a's\n",
      "1           able\n",
      "2          about\n",
      "3          above\n",
      "4      according\n",
      "5    accordingly\n",
      "6         across\n",
      "7       actually\n",
      "8          after\n",
      "9     afterwards\n",
      "10         again\n",
      "11       against\n",
      "12         ain't\n",
      "13           all\n",
      "14         allow\n",
      "15        allows\n",
      "16        almost\n",
      "17         alone\n",
      "18         along\n",
      "19       already\n",
      "20          also\n",
      "21      although\n",
      "22        always\n",
      "23            am\n",
      "24         among\n",
      "25       amongst\n",
      "26            an\n",
      "27           and\n",
      "28       another\n",
      "29           any\n",
      "..           ...\n",
      "540        whole\n",
      "541         whom\n",
      "542        whose\n",
      "543          why\n",
      "544         will\n",
      "545      willing\n",
      "546         wish\n",
      "547         with\n",
      "548       within\n",
      "549      without\n",
      "550        won't\n",
      "551       wonder\n",
      "552        would\n",
      "553        would\n",
      "554     wouldn't\n",
      "555            x\n",
      "556            y\n",
      "557          yes\n",
      "558          yet\n",
      "559          you\n",
      "560        you'd\n",
      "561       you'll\n",
      "562       you're\n",
      "563       you've\n",
      "564         your\n",
      "565        yours\n",
      "566     yourself\n",
      "567   yourselves\n",
      "568            z\n",
      "569         zero\n",
      "\n",
      "[570 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IsNotStopWord(w):\n",
    "    wordlist=[]\n",
    "    wordlist.append(w)\n",
    "    if (w == u'') | (w == u'a'):   # handle NL and 'a' as a special case\n",
    "        return False\n",
    "    return(~any(stopwords.isin(wordlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('him-let', 1)\n",
      "('fire!', 1)\n",
      "('matter;', 1)\n",
      "('head', 4)\n",
      "('dark', 1)\n",
      "('avoided', 1)\n",
      "('swore', 2)\n",
      "(\"fear'd\", 2)\n",
      "('choked', 2)\n",
      "('tents', 2)\n",
      "('ye', 9)\n",
      "('hart', 2)\n",
      "('cares', 1)\n",
      "('law', 1)\n",
      "('behold', 4)\n",
      "('proof', 3)\n",
      "('chanced', 4)\n",
      "('pluck', 4)\n",
      "('pindarus', 17)\n",
      "('fall', 10)\n"
     ]
    }
   ],
   "source": [
    "# your turn\n",
    "\n",
    "wordcount = (juliusrdd\n",
    "        .flatMap(lambda l: l.lower().split(' '))\n",
    "        .map(lambda w: w.strip(\".\").strip(\",\").strip('\"').strip(\"'\"))\n",
    "\n",
    "        .filter(lambda w: IsNotStopWord(w))\n",
    "        .map(lambda w: (w,1))\n",
    "        .reduceByKey(lambda a,b: a+b)\n",
    "        .collect())\n",
    "\n",
    "for i in range(20):\n",
    "    print(wordcount[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the word counting MapReduce code you've seen before. Count the number of times each word occurs and print the top 20 results as a list of tuples of the form `(word, count)`. *Hint: use `takeOrdered()` instead of `take()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your turn\n",
    "\n",
    "wordcounts = (juliusrdd\n",
    "        .flatMap(lambda l: l.lower().split(' '))\n",
    "        .map(lambda w: w.strip(\".\").strip(\",\").strip('\"').strip(\"'\"))\n",
    "        .filter(lambda w: IsNotStopWord(w))\n",
    "        .map(lambda w: (w,1))\n",
    "        .reduceByKey(lambda a,b: a+b))\n",
    "top20 = wordcounts.takeOrdered(20, key = lambda x: -x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a bar graph. For each of the top 20 words on the X axis, represent the count on the Y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brutus</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cassius</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caesar</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antony</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thou</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1\n",
       "0   brutus  355\n",
       "1  cassius  218\n",
       "2   caesar  205\n",
       "3   antony  123\n",
       "4     thou  112"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(top20)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAE2CAYAAAB7gwUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm4HFW97vHvSwiDzEhEphBQEAMqYgBBVBBlEgQUIQ6I\nXDyIIsJ1BNQjDnAQQRwOoijzIESECyIyzzKEJIxB0ChECFNAIAExkvC7f6zVpNKpvXdVD3t3Nu/n\nefrp7upaq1d1V/evak2liMDMzKzZYkNdADMz600OEGZmVsoBwszMSjlAmJlZKQcIMzMr5QBhZmal\nHCB6kKQjJJ011OUYCpI+IemKIS5DSHpji2kfkvT+Tpfp1aK470saLel5SSOGulyvVg4QA5B0mKQ/\nNi37ax/Lxg9CebaW9HL+4TRuv+/2+w6WiDg7Irbr5ntIWlHSKZIelzRb0l8kHdrN9xwO8r53XQvp\nTpP0/brpIuIfEbFsRMyrm7ZCmT4uaVL+/Twm6Y+Stur0+yzqHCAGdgOwZeMoRtJqwEjg7U3L3pjX\nrUxJK9/Bo/mH07jt0kf+i7eQ96vB8cCywJuBFYAPAdOGtEQ94NVypC7pS8CPgaOAVYHRwAmk/WDI\ntfG/0HE9UYgedzspIGycn78buBZ4oGnZ3yLiUQBJW0q6XdJz+X7LRmaSrpN0pKQ/Af8C1pW0jqTr\n89HslcAqrRQ0n56fL+ksSbOAT0taTNKhkv4m6WlJEyStXEizt6Tp+bVvFKtImo/88hHkI4Xnq0v6\nnaSZkh6U9MWmskyQdEberqmSxhVeX0vSBTnt05L+Ny//tKSbCuttIOlKSf+U9ICkPQuv7STpvpz/\nDElfqfhRbQqcExHPRMTLEXF/RJzftM7781nhs5JOkKT8nm+QdE0u81OSzpa0Yh/fx5vz5/KxCp/X\nZvmIdpakJyT9KC8fo1Tltb+kR/PR7lcK6Qb6fn+rdKb0nKQbJG1YeO00SSdKulTSC8A2kpaUdKyk\nf+Ry/ELS0iXbJknHS3oyl/keSRsN9ME370N5WWm1XGHbFy9bTwtWRy2V9/un83d2u6RVS/JcAfgu\ncGBEXBARL0TESxFxSUR8rfBd3JLzeUzS/0paopBH7X1S0kqSLsnf/TP58ZqFdAv9Lwz0WQ4GB4gB\nRMR/gNuA9+RF7wFuBG5qWnYDQP5x/gH4KfBa4EfAHyS9tpDt3sD+wHLAdOAcYDIpMHwP2KeNIu8K\nnA+sCJwNHATsBrwXWB14hnS0hKSxwIm5PKvn8q65cJYLUzrC+T1wF7AGsC1wiKTtC6t9CDg3l+Vi\noBEERgCXkLZ9TE5/bsl7LANcSfp8XgeMB36eyw1wMvDZiFgO2Ai4pkrZgVuBIyXtK2m9PtbZmRRI\n3grsCTS2S8D/kD6vNwNrAUeUlH0T4HLgoIj4TYXP6yfATyJieeANwISmLLcB1gO2A75e+KPs8/vN\n/pjTvQ6YQtonij4OHEnaF28CjgbWJx38vDGX9b8BIuK6iNg6p9uOtN+vTzoL2xN4uvlzGET75HKs\nRdqPDwBeLFlvC2Ap4MJ+8poH/F/S73EL0nf1eWhrn1wMOBVYm3TG8iL591DQ/L8w9CLCtwFupD+A\nC/Pju0g/uB2alu2TH+8NTGxKfwvw6fz4OuC7hddGA3OBZQrLzgHO6qMsWwMvA88WbnsWynlD0/p/\nBrYtPF8NeAlYnPTDP7fw2jLAf4D35+enAd9veu9H8uPNgX80vddhwKmFslxVeG0s8GJ+vAUwE1i8\nZPs+DdyUH+8F3Nj0+i+Bb+fH/wA+Cyxf8/tcGjicFJRfIlUv7Vh4PYCtCs8nAIf2kdduwB2F5w8B\n3wEeAbYuLB/o87ohp1ulaZ0xuTwbFJYdA5w80PdbUtYVc14rFL7fMwqvC3gBeENh2RbAgyV5vQ/4\nC/BOYLEBPu9X9qPiPtT0mTX2uSPI+35h2xdvXq9k3f8D3Ay8dYCyfAJ4vOb+cgjzf+sd2SdJAfiZ\nwvPrKPwv9MrNZxDV3ABslc8ORkXEX0k745Z52UbMb39YnYWj/3TSkVjDw4XHq5N2lBea1u/PoxGx\nYuFWPNp8uGndtYEL8+nys6Q/lHmkutfVi+vnMlQ9ClwbWL2Rb8778Jxvw+OFx/8ClsrVBWsB0yNi\nboX32LzpPT4BvD6//hFgJ2C6UhXdFlUKHhEvRsRREfEO0tHmBOC3xaqZkrIvCyBpVUnn5uqDWcBZ\nLFwleABwc0Rc17Qt/X1e+5GOxu/P1SM7N+VZ/F6nk767Rr6l36+kEZKOztVPs0h/sDSVt5jvKOA1\nwORCfpfl5QuIiGtIR8AnAE9KOknS8s3rDaIzSWds5ypVxR0jaWTJek8Dq6if9jlJ6+cqoMfz53YU\n8z+zlvZJSa+R9Eul6txZpP+LFbVgu0/zb3fIOUBUcwvp9PW/gD8BRMQs4NG87NGIeDCv+yhpJyoa\nDcwoPC9OofsYsFI+dS2u36rm6XkfJh0dFwPKUhExI7/3Wo0VJb2G9IfZ8ALpD6Ph9YXHD5OOLIv5\nLhcRO1Uo48PA6P5+pIX1rm96j2Uj4nMAEXF7ROxKOtX/fyxcLTOg/D0eRTp7WqdCkqNIn/FbIlUH\nfZJ05F10AGn7jm/alj4/r4j4a0R8LG/LD4Dzm/aJtQqPR5P2s0a+fX2/HydVOb6ftP+OyWmK5S3u\nL0+Rqj42LOS1QkQsW/ZBRMRPc5AdSwpuXy1br8kC+1T+g1woAFVJS2F/jNSO8J2IGAtsSaoi/FRJ\nHrcAc0hnfn05EbgfWC9/x4cz/zNrdZ/8MvAmYPOcZ6N6uq/voic4QFQQES8Ck4AvkdofGm7Ky4q9\nly4F1lfqRre4pL1IP6BL+sh7es77O5KWUOpqV9orqUW/INW3rw0gaZSkXfNr5wM7S9oqN8J9lwX3\niTuBnSStLOn1pFPthonAbElfl7R0PlrdSNKmFco0kRScjpa0jFID47tK1ruE9FnuLWlkvm2q1Pi7\nhNKYiRUi4iVgFqnqbUCSvpXzWULSUsDBpKq6ByokXw54HnhO0hqU/ynOJlVBvkfS0YVt7vPzkvRJ\nSaMiolF9SNP2fCsfhW4I7Aucl5f39/0uR/ozfJr0x3pUfxuW3/tXwPGSXpfzW6OpXYm8fFNJm+ej\n9BeAf1Pt8/8L6UzygzntN4ElK6SDtD+Oz/vBOGCPQnm2kfSWHHBmkarZFipPRDxHqlo9QdJu+TMd\nKWlHScfk1ZbLeTwvaQPgc4UsWt0nlyMF32fzmeq3K27zkHKAqO560lHBTYVlN+ZlrwSIiHiadPTy\nZdIP82vAzhHxVD95f5xUR/1P0o5zRgfL/RNSA/EVkmaTGmg3z2WdChxIavN4jNTAWexhciapfeUh\n4Arm/ykRqW/6zqS61AdJR5+/Jh2p9iun3YXUCPqP/J57law3m9QYOp50xPw46ei68YeyN/BQPmU/\ngHSqX0WQGgyfyvl+APhgRDxfIe13gE2A50idES4ofYOIZ3O+O0r6XoXPawdgqqTnSd/Z+Hxg0nA9\nqa3kauDYiGgMJuzz+yXtR9NJZ6/35dcG8vX8Prfmz/Uq0pFvs+VJweSZ/B5PAz/sJ9/UuJP+oD+f\nt30GKbg80k+6om+RGvCfIX0P5xReez3pgGcWqZrtetL+u3BBIo4jHdh9k9QW9jDwBdIRP8BXSL/J\n2Xkbi/t9q/vkj0ltX0+RvofLKm7zkFJuIDEDUldC4DMRcdVQl8VSV09SQBlZoc2mJ0m6gNR54sdD\nXRarx2cQZtY1uRpuK1I1qi1iHCDMrCskfR64g9SV96aB1rfe4yomMzMr5TMIMzMrtUhP5rbKKqvE\nmDFjhroYZmaLlMmTJz8VEQOOP1mkA8SYMWOYNMltX2ZmdUiqNNeTq5jMzKyUA4SZmZVygDAzs1IO\nEGZmVsoBwszMSjlAmJlZKQcIMzMr5QBhZmaluhYg8kVgJkq6S9JUSd/Jy49Qulzjnfm2UyHNYZKm\nSXqg7CIlZmY2eLo5knoO8L6IeD5fOeomSX/Mrx0fEccWV5Y0lnQRjg1J19u9StL6+UIr/Zp54lm1\nCzfqc5+sncbM7NWka2cQkTSu0DUy3/qbOnZX4NyImJOv7zwN2Kxb5TMzs/51tQ0iX3f3TuBJ4MqI\nuC2/dJCkuyWdImmlvGwN0qX/Gh7Jy8zMbAh0NUBExLyI2BhYE9hM0kbAicC6pGvzPgYcVydPSftL\nmiRp0syZMzteZjMzSwalF1O+gPu1wA4R8UQOHC+TLgjeqEaaAaxVSLZmXtac10kRMS4ixo0aNeBs\ntWZm1qJu9mIaJWnF/Hhp4APA/ZJWK6y2O3BvfnwxMF7SkpLWAdYDJnarfGZm1r9u9mJaDThd0ghS\nIJoQEZdIOlPSxqQG64eAzwJExFRJE4D7gLnAgVV6MJmZWXd0LUBExN3A20uW791PmiOBI7tVJjMz\nq84jqc3MrJQDhJmZlXKAMDOzUg4QZmZWygHCzMxKOUCYmVkpBwgzMyvlAGFmZqUcIMzMrJQDhJmZ\nlXKAMDOzUg4QZmZWygHCzMxKOUCYmVkpBwgzMyvlAGFmZqUcIMzMrJQDhJmZlXKAMDOzUg4QZmZW\nygHCzMxKdS1ASFpK0kRJd0maKuk7efnKkq6U9Nd8v1IhzWGSpkl6QNL23SqbmZkNrJtnEHOA90XE\n24CNgR0kvRM4FLg6ItYDrs7PkTQWGA9sCOwA/FzSiC6Wz8zM+tG1ABHJ8/npyHwLYFfg9Lz8dGC3\n/HhX4NyImBMRDwLTgM26VT4zM+tfV9sgJI2QdCfwJHBlRNwGrBoRj+VVHgdWzY/XAB4uJH8kL2vO\nc39JkyRNmjlzZhdLb2b26tbVABER8yJiY2BNYDNJGzW9HqSzijp5nhQR4yJi3KhRozpYWjMzKxqU\nXkwR8SxwLalt4QlJqwHk+yfzajOAtQrJ1szLzMxsCHSzF9MoSSvmx0sDHwDuBy4G9smr7QNclB9f\nDIyXtKSkdYD1gIndKp+ZmfVv8S7mvRpweu6JtBgwISIukXQLMEHSfsB0YE+AiJgqaQJwHzAXODAi\n5nWxfGZm1o+uBYiIuBt4e8nyp4Ft+0hzJHBkt8pkZmbVeSS1mZmVcoAwM7NSDhBmZlbKAcLMzEo5\nQJiZWSkHCDMzK+UAYWZmpRwgzMyslAOEmZmVcoAwM7NSDhBmZlbKAcLMzEo5QJiZWSkHCDMzK+UA\nYWZmpRwgzMyslAOEmZmVcoAwM7NSDhBmZlbKAcLMzEp1LUBIWkvStZLukzRV0sF5+RGSZki6M992\nKqQ5TNI0SQ9I2r5bZTMzs4Et3sW85wJfjogpkpYDJku6Mr92fEQcW1xZ0lhgPLAhsDpwlaT1I2Je\nF8toZmZ96NoZREQ8FhFT8uPZwJ+BNfpJsitwbkTMiYgHgWnAZt0qn5mZ9W9Q2iAkjQHeDtyWFx0k\n6W5Jp0haKS9bA3i4kOwRSgKKpP0lTZI0aebMmV0stZnZq1vXA4SkZYHfAYdExCzgRGBdYGPgMeC4\nOvlFxEkRMS4ixo0aNarj5TUzs6SrAULSSFJwODsiLgCIiCciYl5EvAz8ivnVSDOAtQrJ18zLzMxs\nCHSzF5OAk4E/R8SPCstXK6y2O3BvfnwxMF7SkpLWAdYDJnarfGZm1r9u9mJ6F7A3cI+kO/Oyw4GP\nSdoYCOAh4LMAETFV0gTgPlIPqAPdg8nMbOh0LUBExE2ASl66tJ80RwJHdqtMZmZWnUdSm5lZKQcI\nMzMr5QBhZmalHCDMzKyUA4SZmZVygDAzs1IOEGZmVsoBwszMSjlAmJlZKQcIMzMr5QBhZmalHCDM\nzKyUA4SZmZXq5nTfi5THT/x+7TSv/9w3u1ASM7Pe4DMIMzMr5QBhZmalHCDMzKyUA4SZmZVygDAz\ns1KVAoSkd1VZZmZmw0fVbq4/AzapsOwVktYCzgBWBQI4KSJ+Imll4DxgDPAQsGdEPJPTHAbsB8wD\nvhgRl1fekiF2/wm7tpRugwMv6nBJzMw6o98AIWkLYEtglKQvFV5aHhgxQN5zgS9HxBRJywGTJV0J\nfBq4OiKOlnQocCjwdUljgfHAhsDqwFWS1o+Iea1smJmZtWegKqYlgGVJgWS5wm0WsEd/CSPisYiY\nkh/PBv4MrAHsCpyeVzsd2C0/3hU4NyLmRMSDwDRgs7obZGZmndHvGUREXA9cL+m0iJje6ptIGgO8\nHbgNWDUiHssvPU6qgoIUPG4tJHskL2vOa39gf4DRo0e3WiQzMxtA1TaIJSWdRGo3eCVNRLxvoISS\nlgV+BxwSEbMkvfJaRISkqFPgiDgJOAlg3LhxtdKamVl1VQPEb4FfAL8mNSBXImkkKTicHREX5MVP\nSFotIh6TtBrwZF4+A1irkHzNvMzMzIZA1XEQcyPixIiYGBGTG7f+EiidKpwM/DkiflR46WJgn/x4\nH+CiwvLxkpaUtA6wHjCx8paYmVlHVT2D+L2kzwMXAnMaCyPin/2keRewN3CPpDvzssOBo4EJkvYD\npgN75rymSpoA3EfqAXWgezCZmQ2dqgGiccT/1cKyANbtK0FE3ASoj5e37SPNkcCRFctkZmZdVClA\nRMQ63S6ImZn1lkoBQtKnypZHxBmdLY6ZmfWKqlVMmxYeL0WqIppCmkrDzMyGoapVTAcVn0taETi3\nKyUyM7Oe0Op03y8AbpcwMxvGqrZB/J7UawnSJH1vBiZ0q1BmZjb0qrZBHFt4PBeYHhGPdKE8ZmbW\nIypVMeVJ++4nzeS6EvCfbhbKzMyGXtUryu1Jmvbio6SRz7dJ6ne6bzMzW7RVrWL6BrBpRDwJIGkU\ncBVwfrcKZmZmQ6tqL6bFGsEhe7pGWjMzWwRVPYO4TNLlwG/y872AS7tTJDMz6wUDXZP6jaQrwH1V\n0oeBrfJLtwBnd7twZmY2dAY6g/gxcBhAvuDPBQCS3pJf26WrpTMzsyEzUDvCqhFxT/PCvGxMV0pk\nZmY9YaAAsWI/ry3dyYKYmVlvGShATJL0X80LJX0G6PeSo2ZmtmgbqA3iEOBCSZ9gfkAYBywB7N7N\ngpmZ2dDqN0BExBPAlpK2ATbKi/8QEdd0vWRmZjakql4P4lrg2i6XxczMekjXRkNLOkXSk5LuLSw7\nQtIMSXfm206F1w6TNE3SA5K271a5zMysmm5Ol3EasEPJ8uMjYuN8uxRA0lhgPLBhTvNzSSO6WDYz\nMxtA1wJERNwA/LPi6rsC50bEnIh4EJgGbNatspmZ2cCGYsK9gyTdnaugVsrL1gAeLqzzSF62EEn7\nS5okadLMmTO7XVYzs1etwQ4QJwLrAhsDjwHH1c0gIk6KiHERMW7UqFGdLp+ZmWWDGiAi4omImBcR\nLwO/Yn410gxgrcKqa+ZlZmY2RAY1QEharfB0d6DRw+liYLykJSWtA6xHuoKdmZkNkarXg6hN0m+A\nrYFVJD0CfBvYWtLGQAAPAZ8FiIipkiYA9wFzgQMjYl63ymZmZgPrWoCIiI+VLD65n/WPBI7sVnnM\nzKweXzbUzMxKOUCYmVkpBwgzMyvlAGFmZqUcIMzMrJQDhJmZlXKAMDOzUg4QZmZWygHCzMxKOUCY\nmVmprk21YfVd96sPtpRu6//6Q4dLYmbmMwgzM+uDA4SZmZVygDAzs1IOEGZmVsoBwszMSjlAmJlZ\nKXdzHWbOP3WH2mn22PeyLpTEzBZ1PoMwM7NSDhBmZlaqawFC0imSnpR0b2HZypKulPTXfL9S4bXD\nJE2T9ICk7btVLjMzq6abZxCnAc0V4ocCV0fEesDV+TmSxgLjgQ1zmp9LGtHFspmZ2QC6FiAi4gbg\nn02LdwVOz49PB3YrLD83IuZExIPANGCzbpXNzMwGNthtEKtGxGP58ePAqvnxGsDDhfUeycvMzGyI\nDFkjdUQEEHXTSdpf0iRJk2bOnNmFkpmZGQx+gHhC0moA+f7JvHwGsFZhvTXzsoVExEkRMS4ixo0a\nNaqrhTUzezUb7ABxMbBPfrwPcFFh+XhJS0paB1gPmDjIZTMzs4KujaSW9Btga2AVSY8A3waOBiZI\n2g+YDuwJEBFTJU0A7gPmAgdGxLxulc3MzAbWtQARER/r46Vt+1j/SODIbpXHzMzq8UhqMzMr5QBh\nZmalHCDMzKyUA4SZmZVygDAzs1IOEGZmVsoBwszMSjlAmJlZKQcIMzMr5QBhZmalHCDMzKyUA4SZ\nmZXq2mR9tuj65Znb107z2b0v70JJzGwoOUBYxx0xoX6AAThiz/lBZseLPtJSHn/c9XctpTOzhbmK\nyczMSjlAmJlZKQcIMzMr5QBhZmalHCDMzKyUezHZsLXThd+vnebS3b/ZhZKYLZqGJEBIegiYDcwD\n5kbEOEkrA+cBY4CHgD0j4pmhKJ+ZmQ3tGcQ2EfFU4fmhwNURcbSkQ/Pzrw9N0cySD15wYu00f/jw\n5155vPP5Z7f0vpfs8YmW0pl1Ui+1QewKnJ4fnw7sNoRlMTN71RuqABHAVZImS9o/L1s1Ih7Ljx8H\nVh2aopmZGQxdFdNWETFD0uuAKyXdX3wxIkJSlCXMAWV/gNGjR3e/pGZD7EPn/76ldBfvscsrj3f/\n3U2101/4ka1ael8bPobkDCIiZuT7J4ELgc2AJyStBpDvn+wj7UkRMS4ixo0aNWqwimxm9qoz6AFC\n0jKSlms8BrYD7gUuBvbJq+0DXDTYZTMzs/mGooppVeBCSY33PyciLpN0OzBB0n7AdGDPISibmfVh\nrwum1U5z3off2IWS2GAZ9AAREX8H3lay/Glg28Euj5kNjhMufKKldAfuPr+/yh/Pe6qfNcvtuNcq\nLb2veSS1mb3K3PHr0ubNfr39M6/rQkl6nwOEmVlNjx0zo3aa1b62RhdK0l0OEGZmg+yJH09uKd2q\nh7zjlcdP/u8VtdO/7gvb1Vq/l0ZSm5lZD3GAMDOzUg4QZmZWygHCzMxKOUCYmVkpBwgzMyvlAGFm\nZqUcIMzMrJQDhJmZlXKAMDOzUg4QZmZWygHCzMxKOUCYmVkpBwgzMyvlAGFmZqUcIMzMrJQDhJmZ\nlXKAMDOzUj0XICTtIOkBSdMkHTrU5TEze7XqqQAhaQRwArAjMBb4mKSxQ1sqM7NXp54KEMBmwLSI\n+HtE/Ac4F9h1iMtkZvaqpIgY6jK8QtIewA4R8Zn8fG9g84j4QmGd/YH989M3AQ8MkO0qwFNtFKvd\n9MMpj14oQ6/k0Qtl6JU8eqEMvZJHL5ShSh5rR8SogTJZvM1CDLqIOAk4qer6kiZFxLhW36/d9MMp\nj14oQ6/k0Qtl6JU8eqEMvZJHL5ShU3lA71UxzQDWKjxfMy8zM7NB1msB4nZgPUnrSFoCGA9cPMRl\nMjN7VeqpKqaImCvpC8DlwAjglIiY2ma2laujupR+OOXRC2XolTx6oQy9kkcvlKFX8uiFMnQqj95q\npDYzs97Ra1VMZmbWIxwgzMyslAOEmZmVcoAYgKTFJC3fQpotO/Deb2k3D7NeJGmEpGOHuhydIGnl\nkmXrDHIZRkg6u9P59lQvpk6R9FHgsoiYLembwCbA9yNiSsX05wAHAPNIXW+Xl/STiPhhlfQR8bKk\nE4C3t7YFr/i5pCWB04CzI+K5OoklXQss1AshIt5XIe09ZWkLeby1RjneBRwBrE3a55SyiHUrpl8K\n2A/YEFiqUIb/U7UMOZ81CmVo5HFDjfTt7lcXACcDf4yIl+uUvSSvlrdF0vrAicCqEbGRpLcCH4qI\n79cswwdZ+Dv5bpW0ETFP0lZ13q/k/ZcBXsy/t/WBDUif7Us18jgYOBWYDfya9Js9NCKuqFGU30va\nMSJm5TzHAhOAjWrkgaS1gfUi4ipJSwOLR8TsKmnz57m2pCXyNEWdERHD7gbcne+3Aq4DPgjcViP9\nnfn+E8BxwMhGnjXyOBb4CLmnWBvbsh7wP8A04BzgAzXSvqNwexfwI+CYimnXzrdj8u0t+XY0cHTN\nbbifNAHj64DXNm410v8W+B7wN2Af4ArgJzXL8APgIeBS4Pf5dvEg71fvB87O23E08KYW94m2tgW4\nnjTv2R2FZffWLMMvgDOAh4FvA/cAJ9fM40TSOKe9gQ83bjXSTwZeA6yRP4/fkg6k6pThrny/PXAB\nKeBNqZnHB/Nnumz+rU0FNq6Zx3+RDkb/lp+vB1xdM48zch7fAr7UuLWyj72SZzuJe/XW2PHzH+vH\ni8sqpp9KCgq/Bd5b3JFq5DEbeBn4DzArP5/V4vaMIAWbGcCf8x9u5R9SU14TW/ksm5bV/QFV/hMd\n4Pts/EGPBG6tmccDwJJDuV8V8lmBdIb6MHAzsC8wcrC2Bbi9uezkg6IaedzddL8scGPNPE4tuZ1S\nI/2UfH8Q8LU2t+MnwO5tfKe75e/yHmD9FtLfCSzR9J3cUzOPb5fdWt1PImJ4VjEBMyT9EvgA8INc\nTVOnveWXpCOSu4Ab8qnfrDoFiIjl6qxfJp/670s6QrkS2CUipkhaHbiFdMTTX/pi3ehipKObFeoX\nQ++KiD/lJ1tSv+3qWkk/JJV3TmNhVKyaARpVBs9K2gh4nHQ2UsffSYFlzkAr9qPd/QpJrwU+STpq\nvoN0RrEV6cxo64rZtLstT0l6A7kKMU+S+VjNPF7M9//K++PTwGp1MoiIfWu+ZzNJ2oJ0pr9fXjai\nZh6TJV0BrAMcJmk50oFdlTf/GQtWw65AOjv8giQi4os1yjEnIv4jqZH34vRTxVsmIr5TZ/0qhmuA\n2BPYATg2Ip6VtBrw1aqJI+KnwE8Li6ZL2qZuISStRDpVLNbRVq7zBn5GqrM+PCIaP0gi4tFcBz6Q\nyaSdTMBc4EHm/5Cq2g84RdIKOZ9ngFp1/8Dm+b44eVgAA7aFZCflz/KbpCqJZYH/rlmGfwF3Srqa\nBYNUnR9xW/uVpAtJMxCfSQr2jT/l8yRNqlGOdrflQNJI2w0kzSDtF5+s8f4Al0haEfghMIX0ff66\nTgYdaAs5BDgMuDAipkpaF7i2ThlI+/fGwN8j4l85gFcNXM3f2eSa7110vaTDgaUlfQD4PKnqsDJJ\no4CvsXC7UNXf2cJ55lOTYUXS6LLlEfGPiulL/3yiYgNczuMzwMGkCQfvBN4J3NLOlzWUcoAgajaU\n9wpJ+5TYrOMeAAATv0lEQVQtj4jTa+TR7n61TUTU/QMry6ftbcn5LAMsFhUbQvvJZ0lgqbr7hqTr\nSQH2lxHx9rzs3oio1bjbisJ3OS8ihnxCUEmLkYLVdqQDscuBX0eNP+h8JnQe8BVSFeY+wMyI+HrL\n5RqmAaLRA0ekSLoO8EBEbFgx/ZcLT5cCdgb+HDV6zeQybEqqK99Y0gbAURHx4Rp5tNv7ZyTwOeA9\nedF1pB9jnV4eK5DqMht5XA98t86fgaRVgaOA1SNix9zLY4uIOLli+qNIjevP5ucrAV+OiCpnUcV8\nlgZGR8RA1xDpK31b+1XOY0tgDAv2PjqjlfK0qp3vQ1K/+29E9Fvt2ZTX7RGxqaQ7CgHizojYuGL6\ndnrpNQL10xGxR9Uy95FXoyPJWBY8cq/0O+0USZMj4h2S7o7cy7DxGbea57CsYoqIBcYPSNqEdMpW\nNf1xTemPJUX0Ov4dEf+WhKQlI+J+SW+qmcfJwP8lnbrOq5kW0un7SODn+fneedlnauRxCnAvqXql\nkceppB4nVZ2W03wjP/8L6UinUoAAdoyIwxtPIuIZSTuRqpwqkbQLqWfZEsA6kjYmBboPVc2j3f1K\n0pnAG0hnlI3vM0i9TyrrwB/SabT+fezSz2vBAO1iTdptC/lK4fFSpI4cc6skjIht8lH7O2u8X19O\nJR1EHQ9sQ6qiqts2tTOpp17zwWCdMViNA7/HchfkR4GFxmjU0k4L96J0o2aPgKa0K5EuhVonzYXA\niqQzgBuAi4BLa+bRbu+fhXpelS0bII+FeoWULRsgj7Z6zQB3U+i1AywNTK1ZhsmkRsSWu3a2u1+R\neqC11e0553MTsG3+XNbO+9h3B+v76NQNWBe4itSmMiNv19pt5tl2L70W3nNy877QWFYjj2nAW9vZ\nP0g1HSuQxl9cm/f5D7WzbcPyDELSlwpPFyMNaHq0RvriILERwCigcvsDQETsnh8ekU9nVwAuq5MH\n7ff+mSfpDRHxN4DciFf3TORFSVtFxE05j3cxvwdLVS/kxr/GkeI7gTr11WcDV0s6NT/fF6hV3w68\nFBHPNXqJZLUGq7W7X5HOxF5P/R5DzZaOiKslKSKmk/axyVRvuG/3+yCna3mg3PzV4/3FthDVGIHc\noV56V0v6CHBB5H/ZFszJZyN/VbpcwQxSR4o6HiYdsLRc5x8Rl+SHz5HOZNo2LAMEUOxiOhf4A/C7\nGul3bkr/RERUOnVtyKfOj0TEHNLp4hjSoJ46oxzb7f3zVVKQ+Xsuw9pU76HR8Dng9EIvpn+SGr/q\n+DKp99EbJP2JFHA/WjVxRPxA0l2kgWYA34uIulV+UyV9HBiRq2i+SOq3XkdL+5Wk35O+t+WA+yRN\nZMGAX7maK2v3D+lLLPx91KqHl/QL0v68Dan30h7AxDp5kD67TSLihcKy80l/9FV0opfeZ0mfx1xJ\n/6a1qp2DSZ/FF0nVRNtQ/zfyNeDS3HBf3Dd+VDWDDvQKWzjPNgJWz5L00Yj47UDLStItHxGzVDK3\nCkBE/LNGGe4k/bGPIY14vQjYMCJ2qppHJ+QeJo22jwdywGoln+UBIk8n0EL6xXM5lMtRp6G8OKXC\nm3I+dadUeA2pzn27vOhyUqBpZ1xE1fd+L2m7f0D6I3jlJeAHEbF5acK+89uUVF21IukPaXlSI/5t\nNfJo+fvI6e+OiLcW7pclfSfvrpB2A9KZxzEs2E14eeCrUaPRv5dIek1E/KvFtFcAz5MG2r1yZhs1\nxjZ0o1fYcD2DOIw0CnqgZc3OIZ09FI9MGoJUZ1rVy5GukLc78LOI+JmkO2qk70gPItLR2BjSd72x\n0gCeyo2izWXIO2HdXkx/A34YEb8oLLskInbuJ1nRDcC7c++ly0j9z/ciDZCqamy+LZ5vuwIfItX7\nDlT+H0fEIYUzgQUMdAYQEdfnfEY2HhfyXrryFhSyJI2lWJvUCQHgV1TYloLNmL9fbFJ3v6C9gXJv\nIv3OVmTBRu/ZpCkn+iXpfRFxTV89qqJGT6qcX1vjlZQG651MOosbLeltwGcjonIHBlKPsna7974m\nIiY2VaPWqvloNqwChKQdgZ2ANSQVB7otT4UPqvGHFRGdmInxJUkfI51qNn4EI/tZv0xbPYg61Gum\nE72YXgK2kbQ56YfzH9L8OVUp0iCm/YATI+KYfIZWx9mkXi/3UrPtgfRnDKkXVG2SPkfq7bSupLsL\nLy0H/KmFLM8mHSkucLRZozyd2C9aHigXERcBF0naIiJuqfGeDe8FrqG8R1WtnlTqY7wS1atxAX5M\nmsvpYoCIuEvSe/pPspBLJW0X9SYJbNaJEfILaqeFu9duwNtIf8jT833j9mFgpRr5vAtYJj/+JGmS\nu9E1yzKWNBr7Y/n5OsDXa+bRVg8iOtBrpt0y5PUbc+Z8DbgNGE2N+ZxIU1JsAdxKqqaD+vPU3NSB\n/avRmNp4PoJ01DZQuhVIR+u/Yf4kiGsDK7dYjra2pRP7RVN+SwIrtJBuFHA4aVT3KY1bjfQjOlD2\ne0hnDo0JOjcgNVjXyeO2fF/sFdbq3G3/zo9rz91GF3qFDasziIi4C7hL0tlRs1G5yYnA2/Kp4pdJ\nR0Znko5cqpblPklfJ/0ZEhEPkuqg62i3B1Enes10oheTACId+U8hzcZap3/2wbQ/pcK3Jf0aaJ6e\nok51xNWkhvLn8/OlSdvS77U/IlXHPQd8rE6B+9HutrS9X6gw9TnpbGYTSd+LiDrVqBcBN5L+1FoZ\n5/OgpMtIYziuifwvWVMnxis9rDQAMpQGpx5MCsKVRQfmbiMFhVNJv42VSfPH7UPNHphFwypAFPxV\nUlldcdU2hLkREZJ2Bf43Ik7O1RuVqQMDs0jD5c/I7QCQ5kEasHdEh3vNtFSGJq90v4w01/32dfKI\nVB98Q+H530k9RurYl3R0OJL51TJ1B3YtFRGN4EBEPJ8bvwdbS9vS4f3iWxHxW6VrOryfVNX0C+b3\nvKviNdHGNBCkz2Bn0txSJ0u6BDi3cTBT0SO5quz/AVdKeoZUA1HHAaTZYNcgdXu+PJepFkkfojDr\nQczvtlrVRcCzpCq/Ot2v+zRcA0SxW+hSpC6VdY5YZ0s6jFS99J7cpbBu+8ERpIbA6wAi4s585FvH\nrIh4W7EHUcV+4scyv9fMboXljWV1bEsac9DoRvk8sKmkxSKi33YASRtExP2kWVA3aXq58s6vzkxC\ntmlE1D0ybPaCpE0ij0OR9A7qn011Qqvb0sn9onHE/0HgpIj4g6S63SkvkbRTRFxaMx0AkXoMTQAm\n5Ibmn5A6clSe0TU6MF4pIp6iXoeJhUg6mjQ1T+OqcAcrzaJ8WI1s1oyIHdopx0LarcNbVG7UGNlI\nOv3+EvDu/Hw08Kma73drLFwvWfeiQwvV09fcjrL0dctwDmkqhmNJF096gNQb7HbyHPz9pD0p319b\ncrumRhmuIPVv/zOpmu8UUvfQOttxKjC2zX1oU9J0zjeS6nenAe/o5H46GNvSof3iEtK0+H8n9UZa\nktbq3eeRgmxL10zJ+8PPczkmAB+pmf4N5FH6pOnWvwisWDOPdUkzr84EniQdya9bM4+7Wbh9q+53\nchLwlk7ua8PyDKLpaHUx0hlFnW2dTbpi2TzNv5Thb2oWo+WBWYV+4is0deVbnsIRdD/pO9lrZk3S\nYKbnc97fJg0Qew+pO/AxfSWMiP3zwx0j4t9NZRxwOwpeG6ma7+BI3USvl3R7nY0g9U65U9KDpGqV\nxoCoyl1DI+L2/N0Ux5XUGj/QIS1tS4f3i7amPs9WIB15rxMR31WaYbXyNSUkPUTqwDCBNH7ihf5T\nlPodME7SG0l/sBeRDorqjFc6BzgBaJyNjCf9X9Qa30IKtI2xVpVHhGv+zA+LA/sqDYxtaR9vNiwD\nBOlIt6ExwnLPPtYtU+x3fwXpaLluv/uDSAOz5pB2lstJg5qqaKufOGmH/SNpQrdDi+mjxmC/7HUs\neGGal0gjNV+UVHWQ2c2kaSkGWtaXTkxC1vapt6RPNS1qZfxAJ7S6LZ3cL1YD/hARcyRtTRqDUfdz\nOIHUhvI+UkPqbNIfdtXZR98aLQ7cLGh7vBKpLeXMwvOzJNW5TohIZ+h35GoukQ7ADu034XxVxxPV\nNuxGUuf2go9GxHlt5DElIjaRdBBp3ptjJN0VEW/rXEkrlaPVfuKdLMO3SEdGF+VFu5D6ex9HqkLq\nM2hKej2p4e4s4OPMH3i4PPCLiNigYhl2JlXrrEW6iNLywBERUeuCKu1SuoJYw1Kk9pkp0eZ00Ysi\ndWCmgMLvrDjdd+XfWT4L3Y+F26bqTMt/G2kcwzdIF3F6UDVHH0v6AanzxrmkI/m9SBN8/jCXZ8Dg\nm88CtmN+cJwYEY9XLUO3DLsziEjTMXyV1PWtVdLClzKsO31vJxpWpyldZWoMC14/oO4V3VoWEd+T\n9EfS2BCAAyKicSWtgc6otgc+TaqmKs4pM5vU/72qj5L6/t9LGnC3MumIa1ADREQcVHyee7+cO5hl\n6CGNI+8P0/qR90uSRjB/YNco6g38O5N0ffbtSWcgn6Bm91JSj7ADgCNzcFiH+QMjq2rUTnyW+SPt\nRapqqjoDwxRSI/PFNd+7uzrZoNErN+Bo0qjZtUhVEStTY1AS6fTuYvLANtIX/NOaZehEw+rNpN4l\ne5Lmuv8INRvheuHWbpkpmZK5bNkQbNdIUjvEkH/GQ7Dtt5HGddxLakOAmtOnk/7QLwYeAY4kdYD4\naN39gtyYm7+PW1vYlqWBN7XxWewJLJ8ff4s01f8mNfO4n1Qd/jdSg/U91Gyk7sZt2FUxAeTGu3bG\nQXSiDG1f3Uk1rq7ViyR9MiLOUrpCX9n3UWmmSqWZXLeOiGfy85WB66PpAj7dpgXnYlqMNFp+QkRU\nrSseNpSuQncA6TK6v8lH3ntGRK3usrnRf1vSEffVEVH5DEDSxIjYTNINpMb3x0lVM5V/58XxShHR\n0nglzZ+wcCtSO+OxwH9HjUkYJa1dtjzSdO5DZthVMWVjSTvMVqQf9I2kQTyVdKh6qBMNq231E+8B\ny+T7sqmo6xyZHAfcIqkx2eJHSUecg604F9NcYHpEPDIE5Rhy0ZmZAog0Tub+FotxUu5I8k3Smciy\npCP4Oo6g/fFKxTEhv4oWxoQMdSDoy3A9g5hA6lfdGHTycdJcMZV6MqkDF//uRMOqpNmkP9k5pIDT\nylz1Q07S6cDBseA1pY+Leo2JY5k/gdo1EXFf50tqVXXiyLsDZVgnB6Z+lw2Qx60R8c6mhvJXzvor\n5nEJaZqLD5B65r1IOpMZ1E4t3TBcA8R9ETF2oGX9pO9E9VDzn+LKpD7jtRqYc7rmqYiv7ztF7yn+\n+Ppb1qtyoC77oSySAbsTlK5g9z7SlBAdufZAC2WYEhGbNC2bHBFVLziEpJNJc1odSmrj+yIwMiIO\nqJHHa0hdj++JiL/mMSFvifZmZu0Jw7WKaYqkd0bErQBK00xPGiBNUSeqh97aCA6QurpJqvWHqPKp\niG8m1dkuShaTtFJTG8Iis+9FZyZSG27avoRrq9TmQNIm7YxXAl6Z8uOCwvPHaP+ysj1hkfmRVlEY\nUTgSuFnSP/LztalXz/l9pcnpvsz86qFDahanE3+KB5P6Rd8aEdvkH8ZRNfPoBb3ShmCd04lLuLaq\n3YGkr8h/7t/IN2syrKqY+uoJ0FC1IagT1UN51O3hzL+K3UdJfa0r97FuVGvlQUmbRxq1OjUWwUsy\nug1heNEQXsK1UIa2B5J2qEPKsDWsziA62BOg7eqhiDhD0iTm/yl+uIU/xU5MRdwT8rY7KAwfLV/C\ntYMOkPTndjo/kDqynEc6I3mlQ0rHS7qIGlZnEJ3SK/3um8r0XvJUxJEu2Wk2ZCQ9QMklXAezu2Yn\nOj90okPKcDasziA6qOfqzBe1nks27M2s02W7SzrRzteJDinDls8g+uA6c7O+SdqWNNVGO5dwbbcM\nnyK1g0wgdTneg/rtfD0xEWSvcoAws9oknUW6TspUCpc9rTvOpwPl2JI0q2wAk+o2WndqvNJw5Som\nM2tFJy7h2hZJBwOfIY1BEPBLSb+KiJ/1n3IBbXdIGc4cIMysFTdLGjvEVa/7Ae+MfCU5pesy3EKq\nKqpqkR7E2W3+IMysFW1fwrUDxPyJ8siP1ce6fem5Dim9xAHCzFrR9iVcO+BU4DZJF+bnuwEn18mg\nQ+OVhi03UpvZIkvSJqRp/QFujIi6V7WzfjhAmJlZqVrXWTYzs1cPBwgzMyvlAGFWgaTjJR1SeH65\npF8Xnh8n6Ust5n2EpK90opxmneQAYVbNn4AtASQtBqxCmiK6YUsqXA9BknsO2iLDAcKsmpuBLfLj\nDUmzmM6WtJKkJYE3A3dI+qGkeyXdI2kvAElbS7pR0sXkKc8lfUPSXyTdRLoAjlnP8dGMWQUR8aik\nuZJGk84WbgHWIAWN54B7SNcU2Bh4G+kM43ZJN+QsNgE2iogHJb0DGJ/XXRyYAkwezO0xq8IBwqy6\nm0nBYUvgR6QAsSUpQPyJ1B//NxExD3hC0vWkS8bOAiZGxIM5n3cDF+bLXZLPLMx6jquYzKprtEO8\nhVTFdCvpDKJK+8ML3S2aWec5QJhVdzOpGumfETEvIv4JrEgKEjeTriuwl6QR+VrH7wEmluRzA7Cb\npKUlLQfsMjjFN6vHVUxm1d1Dals4p2nZshHxVJ4TaAvgLtL1Cb4WEY9L2qCYSURMkXReXu9J4PZB\nKb1ZTZ5qw8zMSrmKyczMSjlAmJlZKQcIMzMr5QBhZmalHCDMzKyUA4SZmZVygDAzs1L/H9mV2moz\nU5/tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a185d4a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = sns.barplot(data=df,x=0, y=1)\n",
    "ax.set_title('Word Frequencies  Shakespeare\\'s Julius Caesar')\n",
    "ax.set(xlabel='Word', ylabel='Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using partitions for parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make your code more efficient, you want to use all of the available processing power, even on a single laptop. If your machine has multiple cores, you can tune the number of partitions to use all of them! From http://www.stat.berkeley.edu/scf/paciorek-spark-2014.html:\n",
    "\n",
    ">You want each partition to be able to fit in the memory availalbe on a node, and if you have multi-core nodes, you want that as many partitions as there are cores be able to fit in memory.\n",
    "\n",
    ">For load-balancing you'll want at least as many partitions as total computational cores in your cluster and probably rather more partitions. The Spark documentation suggests 2-4 partitions (which they also seem to call slices) per CPU. Often there are 100-10,000 partitions. Another rule of thumb is that tasks should take at least 100 ms. If less than that, you may want to repartition to have fewer partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakesrdd=sc.textFile(\"./sparklect/shakes/*.txt\", minPartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1601',\n",
       " 'AS YOU LIKE IT',\n",
       " '',\n",
       " 'by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'DRAMATIS PERSONAE.',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakesrdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the top 20 words in all of the files that you just read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thou</td>\n",
       "      <td>2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thy</td>\n",
       "      <td>1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thee</td>\n",
       "      <td>1112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  thou  2093\n",
       "1   thy  1548\n",
       "2  good  1173\n",
       "3  thee  1112\n",
       "4   sir  1103"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your turn\n",
    "wordcounts = (shakesrdd\n",
    "        .flatMap(lambda l: l.lower().split(' '))\n",
    "        .map(lambda w: w.strip(\".\").strip(\",\").strip('\"').strip(\"'\"))\n",
    "        .filter(lambda w: IsNotStopWord(w))\n",
    "        .map(lambda w: (w,1))\n",
    "        .reduceByKey(lambda a,b: a+b))\n",
    "top20 = wordcounts.takeOrdered(20, key = lambda x: -x[1])\n",
    "df = pd.DataFrame(top20)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEtCAYAAADwTuwCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xe4XVWd//H3h9A7yBUhEAJIEVARAlIdFJAiVRGCSFEE\nUcaBcUYFy4COWFH8WQaMiBRpoUmQImUQpJPQq4QmCQECCEFkgMD398daJ9k5ueWsU+65yf28nuc+\nd5+191pnnX3Kd6+y91ZEYGZmVmKBblfAzMzmPQ4eZmZWzMHDzMyKOXiYmVkxBw8zMyvm4GFmZsUc\nPIY5ScdK+n2369ENkvaTdGWX6xCS3t1k3ickbdfuOs2LJI3O+3LB/PjPkj7X7XrNzxw8hhBJR0u6\nvC7tkT7Sxg5CfbaR9Lakf1T+Lun08w6WiDgzIj7ayeeQtKykUyQ9I+kVSX+VdFQnn3N+lj+TIelr\nTeY/qfJZfkPSm5XHlw9cQr9lnyPpm62UMS9x8Bharge2kDQCQNJKwELAB+rS3p23bZiSZt7vpyNi\nycrfrn2Uv2ATZQ8HJwBLAu8BlgF2AyZ3tUZDQO3z3IQDgReBA5rJHBGH1T7LwPeAcyuf7Z2arNOw\n5OAxtNxOChYb5sdbA9cCD9elPRoRTwNI2kLS7ZJezv+3qBWWm+7HSboR+CewhqTVJV2Xj4KvAlZo\npqK5u+t8Sb+XNAM4SNICko6S9KikFySNl7R8Jc/+kp7M675R7XaRdKqk71a23UbSlMrjlSVdIGm6\npMcl/VtdXcZLOj2/rvsljamsX1XShTnvC5J+mdMPknRDZbt1JV0l6UVJD0vau7JuZ0kP5PKnSvrP\nBnfVJsBZEfH3iHg7Ih6KiPPrttkutyZfkvQrScrPuaak/811fl7SmZKW7eP9eE/eL/s2sL82lTRR\n0gxJz0r6aU6vdf0cKulpSdOqr7OB9/e83MJ6WdL1ktavrDtV0omSLpP0KvBhSYtIOl7S33I9TpK0\nWF87UtISwF7A4cBa1fe4nSRtLenW/H7cIWnLnN6TX99H8+Nl8ud577x/PwF8S6kVc14n6jakRIT/\nhtAfKVj8e17+JfBZ4Li6tFPy8vLA34H9gQWBffPjd+T1fwb+Bqyf1y8E3Az8FFgE+BDwCvD7Puqy\nDTClj3XHAm8Ce5AOQhYDjgBuAVbJ5f8aODtvvx7wj/yci+Q6zAS2y+tPBb7b23Pn8icB/wUsDKwB\nPAbsUKnL/wE7AyOA7wO35HUjgLtJLYAlgEWBrfK6g4Ab8vISwFPAZ/K++gDwPLBeXj8N2DovLwds\n1OD7eTJwfy53rV7WB/BHYFlgFDAd2DGvezewfd5fPaTW5s8qeZ8AtgM2yu/zLg3ur5uB/fPyksBm\neXl0rs/ZeX+8N9en9h71+f7m9Z8FlsrrfgbcVVl3KvAysGWu36L5PZlA+hwvBVwCfL+ffbl/fh9G\n5G1/UVlXq/uClc/+5wZ4b46l7rOfy3kh79cFSJ+p6cByef2uwNRc5zOq+YFzgG92+zdksP66XgH/\n1b0h6QN9UV6+G1gL2LEu7cC8vD9wW13+m4GD8vKfge9U1o0i/WAvUUk7q/4LVFm3DfA28FLlb+9K\nPa+v2/5BYNvK45VIAWZB0g/ZOZV1SwBv0Fjw+CDwt7rnOhr4XaUuV1fWrQe8lpc3z1/+BXt5fQcx\nO3jsA/ylbv2vgWPy8t+AzwNLF76fiwFfJ/2Yv0nqstqpsj7IwSw/Hg8c1UdZewB3Vh4/AXwbmAJs\nU0kfaH9dn/OtULfN6FyfdStpPwJ+O9D720tdl81lLVN5f0+vrBfwKrBmJW1z4PF+9uXV5OBJOlCa\nDixUV/dWg8cxwG/q0q4D9qk8/g1wL/Bk7fXl9GEVPNxtNfRcD2yVuwN6IuIR4CbSWMjywAbMHu9Y\nmfQBrnoSGFl5/FRleWXg7xHxat32/Xk6Ipat/I3vo2yA1YCLcnP/JdKPzVvAivm5Z22f6/DCAM9d\nLXflWrm57K/ncmueqSz/E1hUaRxmVeDJiJjZwHN8sO459gPeldd/gnQU+qRSt9/mjVQ8Il6LiO9F\nxMbAO0jB4bxqd08vdV8SQNKKSoOwU5W6Bn/P3N2MhwE3RcSf615Lf/vrYGBt4CGlrs5d6sqsvq9P\nkt67Wrm9vr+SRkj6Qe7SmkEKbNTVt1puD7A4MKlS3hU5fS6SVgU+DJyZky4mtV4+1tv2LVgN+HTd\nvhvD7H0AMI70PTw5Il5u8/PPMxw8hp6bSQOrhwA3AkTEDODpnPZ0RDyet32a9GGvGkVqVtdUL5s8\nDVgu9x1Xt29W/SWZnyIdVVeDzaIRMTU/96q1DSUtTvoxrXmV9GNS867K8lOkI9JquUtFxM4N1PEp\nYJQGHtB/Criu7jmWjIgvAETE7RGxO/BO4A+kIFAkv4/fI7W6Vm8gy/dI+/i9EbE08GnSEXvVYaTX\nd0Lda+lzf0XEIxGxb34tPwTOr/tMrFpZHkX6nNXK7ev9/RSwO6m7ZxlSS4C6+lY/L88DrwHrV8pa\nJtJAdm/2J/1eXSLpGVI33KKkAfR2eooUFKqvcYmIOAFA0kLASaSW1JGSqt+/YXWJcgePISYiXgMm\nAl8G/lJZdUNOq86yugxYW9KnJC0oaR9Sl80f+yj7yVz2tyUtLGkrUh9uu5wEHFf7QuUBxt3zuvOB\nXSRtJWlh4DvM+fm7C9hZ0vKS3gUcWVl3G/CKpK9JWiwf5W4gaZMG6nQbKXD9QNISkhatDYDW+SNp\nX+4vaaH8t0keiF5Y6ZyQZSLiTWAGqTtvQJK+lctZWNKipHGDl0iTIAayFGmc6GVJI4Gv9LLNK6Ru\nzQ9J+kHlNfe5vyR9WlJPRNS6JKl7Pd+StHge8P4McG5O7+/9XQp4ndSaXJwU+PqUn/s3wAmS3pnL\nGylphz6yHEjqatuw8vcJ0mfmHX3kacZpwCclbZv322J5uXYwcyzp/f8s8CvgNM2exfgsaXxpWHDw\nGJquIx0V3lBJ+0tOmxU8IuIFYBfgP0hf2q+SBk2f76fsT5H6xF8k9e+e3sZ6/z/SAOiVkl4hDa5+\nMNf1ftIsmbNIP+Z/J/XV15xBGs95AriS2T9YRMRbpNe5IfA46aj1ZNIRbr9y3l1Jg89/y8+5Ty/b\nvQJ8FBhLOtJ+hnRUvkjeZH/gidwlcxipS6sRAfwu1/lp0gD4xyLiHw3k/TZpMPxl4FLgwl6fIOKl\nXO5Okv67gf21I3C/pH+Q3rOx+aCl5jrS2Mw1wPERUTuRss/3l/Q5epLU6n0grxvI1/Lz3JL369XA\nOvUbSdqM1ML+VUQ8U/mbkPPv28BzNSQiHiMFpW+T9tuTpIC/gNJMxi+QxhQD+G9SF+O/5+zjgE1y\nd9c57arTUKW0D8wGn6QnSIOaV3e7Lpam6pKCzUINjBHZMOeWh5mZFXPwMDOzYu62MjOzYm55mJlZ\nMQcPMzMrNt9eCXWFFVaI0aNHd7saZmbzlEmTJj0fEb2e6V813waP0aNHM3HixG5Xw8xsniJpoEsW\nAe62MjOzJjh4mJlZMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2IOHmZmVmy+PUkQYPqJv28q\nX88XPt3mmpiZzV/c8jAzs2IOHmZmVszBw8zMijl4mJlZMQcPMzMr1rHgIWlVSddKekDS/ZKOyOnL\nS7pK0iP5/3KVPEdLmizpYUk7VNI3lnRvXvdzSepUvc3MbGCdbHnMBP4jItYDNgMOl7QecBRwTUSs\nBVyTH5PXjQXWB3YE/kfSiFzWicAhwFr5b8cO1tvMzAbQseAREdMi4o68/ArwIDAS2B04LW92GrBH\nXt4dOCciXo+Ix4HJwKaSVgKWjohbIiKA0yt5zMysCwZlzEPSaOADwK3AihExLa96BlgxL48Enqpk\nm5LTRubl+nQzM+uSjgcPSUsCFwBHRsSM6rrckog2PtehkiZKmjh9+vR2FWtmZnU6GjwkLUQKHGdG\nxIU5+dncFUX+/1xOnwqsWsm+Sk6bmpfr0+cSEeMiYkxEjOnpGfD+7WZm1qROzrYS8FvgwYj4aWXV\nBODAvHwgcHElfaykRSStThoYvy13cc2QtFku84BKHjMz64JOXhhxS2B/4F5Jd+W0rwM/AMZLOhh4\nEtgbICLulzQeeIA0U+vwiHgr5/sicCqwGHB5/jMzsy7pWPCIiBuAvs7H2LaPPMcBx/WSPhHYoH21\nMzOzVvgMczMzK+bgYWZmxRw8zMysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bg\nYWZmxRw8zMysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIp18k6Cp0h6TtJ9lbRzJd2V/56o3SRK0mhJ\nr1XWnVTJs7GkeyVNlvTzfDdBMzProk7eSfBU4JfA6bWEiNintizpJ8DLle0fjYgNeynnROAQ4Fbg\nMmBHfCdBM7Ou6ljLIyKuB17sbV1uPewNnN1fGZJWApaOiFsiIkiBaI9219XMzMp0a8xja+DZiHik\nkrZ67rK6TtLWOW0kMKWyzZScZmZmXdTJbqv+7MucrY5pwKiIeEHSxsAfJK1fWqikQ4FDAUaNGtWW\nipqZ2dwGveUhaUHg48C5tbSIeD0iXsjLk4BHgbWBqcAqleyr5LReRcS4iBgTEWN6eno6UX0zM6M7\n3VbbAQ9FxKzuKEk9kkbk5TWAtYDHImIaMEPSZnmc5ADg4i7U2czMKjo5Vfds4GZgHUlTJB2cV41l\n7oHyDwH35Km75wOHRURtsP2LwMnAZFKLxDOtzMy6rGNjHhGxbx/pB/WSdgFwQR/bTwQ2aGvlzMys\nJT7D3MzMijl4mJlZMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2IOHmZmVszBw8zMijl4mJlZ\nMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2Lduof5POOZE7/bVL53feGbba6JmdnQ0ck7CZ4i\n6TlJ91XSjpU0VdJd+W/nyrqjJU2W9LCkHSrpG0u6N6/7eb4drZmZdVEnu61OBXbsJf2EiNgw/10G\nIGk90u1p1895/qd2T3PgROAQ0n3N1+qjTDMzG0QdCx4RcT3w4oAbJrsD50TE6xHxOOl+5ZtKWglY\nOiJuiYgATgf26EyNzcysUd0YMP+SpHtyt9ZyOW0k8FRlmyk5bWRerk83M7MuGuzgcSKwBrAhMA34\nSTsLl3SopImSJk6fPr2dRZuZWcWgBo+IeDYi3oqIt4HfAJvmVVOBVSubrpLTpubl+vS+yh8XEWMi\nYkxPT097K29mZrMMavDIYxg1ewK1mVgTgLGSFpG0Omlg/LaImAbMkLRZnmV1AHDxYNbZzMzm1rHz\nPCSdDWwDrCBpCnAMsI2kDYEAngA+DxAR90saDzwAzAQOj4i3clFfJM3cWgy4PP+ZmVkXdSx4RMS+\nvST/tp/tjwOO6yV9IrBBG6tmZmYt8uVJzMysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEH\nDzMzK+bgYWZmxRw8zMysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxToW\nPCSdIuk5SfdV0n4s6SFJ90i6SNKyOX20pNck3ZX/Tqrk2VjSvZImS/p5vh2tmZl1USdbHqcCO9al\nXQVsEBHvA/4KHF1Z92hEbJj/DquknwgcQrqv+Vq9lGlmZoOsY8EjIq4HXqxLuzIiZuaHtwCr9FeG\npJWApSPilogI4HRgj07U18zMGtfNMY/PApdXHq+eu6yuk7R1ThsJTKlsMyWnmZlZFy3YjSeV9A1g\nJnBmTpoGjIqIFyRtDPxB0vpNlHsocCjAqFGj2lVdMzOrM+gtD0kHAbsA++WuKCLi9Yh4IS9PAh4F\n1gamMmfX1io5rVcRMS4ixkTEmJ6eng69AjMzayh4SNqykbQGytkR+CqwW0T8s5LeI2lEXl6DNDD+\nWERMA2ZI2izPsjoAuLj0ec3MrL0abXn8osG0WSSdDdwMrCNpiqSDgV8CSwFX1U3J/RBwj6S7gPOB\nwyKiNtj+ReBkYDKpRVIdJzEzsy7od8xD0ubAFkCPpC9XVi0NjOgvb0Ts20vyb/vY9gLggj7WTQQ2\n6O+5zMxscA00YL4wsGTebqlK+gxgr05VyszMhrZ+g0dEXAdcJ+nUiHhykOpkZmZDXKNTdReRNA4Y\nXc0TER/pRKXmNw/9avfiPOse7nkBZjZ0NRo8zgNOIg1cv9W56piZ2byg0eAxMyJO7GhNzMxsntHo\nVN1LJH1R0kqSlq/9dbRmZmY2ZDXa8jgw//9KJS2ANdpbHTMzmxc0FDwiYvVOV8TMzOYdDQUPSQf0\nlh4Rp7e3OmZmNi9otNtqk8ryosC2wB2k+2vYIPjzbz5WnGebQy6dtXz+75q7h9Zen7miqXxmNn9r\ntNvqS9XH+fax53SkRmZmNuQ1e0n2VwGPg5iZDVONjnlcQppdBemCiO8BxneqUmZmNrQ1OuZxfGV5\nJvBkREzpa2MzM5u/NdRtlS+Q+BDpyrrLAW90slJmZja0NXonwb2B24BPAnsDt0ryJdnNzIapRgfM\nvwFsEhEHRsQBwKbAt/rLIOkUSc9Juq+StrykqyQ9kv8vV1l3tKTJkh6WtEMlfWNJ9+Z1P8+3ozUz\nsy5qNHgsEBHPVR6/0EDeU4H6kwuOAq6JiLWAa/JjJK0HjAXWz3n+p3ZPc+BE4BDSfc3X6qVMMzMb\nZI0Gjysk/UnSQZIOAi4FLusvQ0RcD7xYl7w7cFpePg3Yo5J+TkS8HhGPk+5XvqmklYClI+KWiAjS\nSYl7YGZmXTXQPczfDawYEV+R9HFgq7zqZuDMJp5vxYiYlpefAVbMyyOBWyrbTclpb+bl+nQzM+ui\ngVoePyPdr5yIuDAivhwRXwYuyuuallsSMeCGBSQdKmmipInTp09vZ9FmZlYxUPBYMSLurU/MaaOb\neL5nc1cU+X9tHGUqsGplu1Vy2tS8XJ/eq4gYFxFjImJMT09PE9UzM7NGDBQ8lu1n3WJNPN8EZt8b\n5EDg4kr6WEmLSFqdNDB+W+7imiFpszzL6oBKHjMz65KBgsdESYfUJ0r6HDCpv4ySziaNjawjaYqk\ng4EfANtLegTYLj8mIu4nXe7kAeAK4PCIqN0r/Yuke6dPBh4FLm/wtZmZWYcMdHmSI4GLJO3H7GAx\nBlgY2LO/jBGxbx+rtu1j++OA43pJnwhsMEA9zcxsEPUbPCLiWWALSR9m9g/4pRHxvx2vmZmZDVmN\n3s/jWuDaDtfFzMzmEc3ez8PMzIYxBw8zMyvm4GFmZsUcPMzMrJiDh5mZFWv0NrRm/PqMHQbeqM7n\n9/9TB2piZt3mloeZmRVz8DAzs2IOHmZmVszBw8zMinnA3AbVsePLB92P3duD7mZDjVseZmZWzMHD\nzMyKDXrwkLSOpLsqfzMkHSnpWElTK+k7V/IcLWmypIcllfd7mJlZWw36mEdEPAxsCCBpBOme5BcB\nnwFOiIjjq9tLWg8YC6wPrAxcLWntyp0GbZjZ6eJPFOe5fPcLOlATs+Gr291W2wKPRsST/WyzO3BO\nRLweEY+Tbke76aDUzszMetXt4DEWOLvy+EuS7pF0iqTlctpI4KnKNlNympmZdUnXgoekhYHdgPNy\n0onAGqQurWnAT5oo81BJEyVNnD59etvqamZmc+pmy2Mn4I58n3Qi4tmIeCsi3gZ+w+yuqanAqpV8\nq+S0uUTEuIgYExFjenp6Olh1M7PhrZvBY18qXVaSVqqs2xO4Ly9PAMZKWkTS6sBawG2DVkszM5tL\nV84wl7QEsD3w+UryjyRtCATwRG1dRNwvaTzwADATONwzrczMuqsrwSMiXgXeUZe2fz/bHwcc1+l6\nmZlZY7o928rMzOZBDh5mZlbMwcPMzIo5eJiZWTHfz8OGnZ0v+m5T+S7b85ttronZvMvBw6wJH7vw\nxOI8l378Cx2oiVl3uNvKzMyKOXiYmVkxd1uZdcku559ZnOePe+3XgZqYlXPwMJuH7Xb+JcV5Juy1\nawdqYsONu63MzKyYg4eZmRVz8DAzs2Ie8zAbxva84Iam8l30ia3aXBOb17jlYWZmxRw8zMysWLfu\nJPgE8ArwFjAzIsZIWh44FxhNupPg3hHx97z90cDBeft/i4g/daHaZtaLfS6cXJzn3I+/uwM1scHU\nzTGPD0fE85XHRwHXRMQPJB2VH39N0nrAWGB9YGXgaklr+1a0ZvOPX130bHGew/dcsQM1sUYNpW6r\n3YHT8vJpwB6V9HMi4vWIeByYDGzahfqZmVnWreARpBbEJEmH5rQVI2JaXn4GqB1WjASequSdktPM\nzKxLutVttVVETJX0TuAqSQ9VV0ZESIrSQnMgOhRg1KhR7ampmZnNpSstj4iYmv8/B1xE6oZ6VtJK\nAPn/c3nzqcCqleyr5LTeyh0XEWMiYkxPT0+nqm9mNuwNevCQtISkpWrLwEeB+4AJwIF5swOBi/Py\nBGCspEUkrQ6sBdw2uLU2M7OqbnRbrQhcJKn2/GdFxBWSbgfGSzoYeBLYGyAi7pc0HngAmAkc7plW\nZlZ1+bnPD7xRL3baZ4U212T4GPTgERGPAe/vJf0FYNs+8hwHHNfhqpnZMHbnyc8NvFEvPvC5d7a5\nJvOGoTRV18zM5hG+MKKZWZtM+1Gvc3n6tdJX580zD9zyMDOzYg4eZmZWzMHDzMyKOXiYmVkxBw8z\nMyvm2VZmZkPIsz+bVJxnxSM37kBN+ueWh5mZFXPLw8xsPvLcL69sKt87//WjRdu75WFmZsUcPMzM\nrJiDh5mZFXPwMDOzYg4eZmZWzMHDzMyKdeM2tKtKulbSA5Lul3RETj9W0lRJd+W/nSt5jpY0WdLD\nknYY7DqbmdmcunGex0zgPyLijnwv80mSrsrrToiI46sbS1oPGAusD6wMXC1pbd+K1sysewa95RER\n0yLijrz8CvAg0N/dUHYHzomI1yPicWAysGnna2pmZn3p6piHpNHAB4Bbc9KXJN0j6RRJy+W0kcBT\nlWxT6D/YmJlZh3UteEhaErgAODIiZgAnAmsAGwLTgJ80UeahkiZKmjh9+vS21tfMzGbrSvCQtBAp\ncJwZERcCRMSzEfFWRLwN/IbZXVNTgVUr2VfJaXOJiHERMSYixvT09HTuBZiZDXPdmG0l4LfAgxHx\n00r6SpXN9gTuy8sTgLGSFpG0OrAWcNtg1dfMzObWjdlWWwL7A/dKuiunfR3YV9KGQABPAJ8HiIj7\nJY0HHiDN1DrcM63MzLpr0INHRNwAqJdVl/WT5zjguI5VyszMivgMczMzK+bgYWZmxRw8zMysmIOH\nmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxRw8zMysmIOHmZkVc/AwM7NiDh5m\nZlbMwcPMzIo5eJiZWbF5JnhI2lHSw5ImSzqq2/UxMxvO5ongIWkE8CtgJ2A90i1r1+turczMhq95\nIngAmwKTI+KxiHgDOAfYvct1MjMbtuaV4DESeKryeEpOMzOzLlBEdLsOA5K0F7BjRHwuP94f+GBE\n/GvddocCh+aH6wAP91PsCsDzLVZtKJQxFOowVMoYCnVoRxlDoQ5DpYyhUIehUsZg1WG1iOgZqKAF\nW6zIYJkKrFp5vEpOm0NEjAPGNVKgpIkRMaaVSg2FMoZCHYZKGUOhDu0oYyjUYaiUMRTqMFTKGAp1\nqJpXuq1uB9aStLqkhYGxwIQu18nMbNiaJ1oeETFT0r8CfwJGAKdExP1drpaZ2bA1TwQPgIi4DLis\njUU21L01D5QxFOowVMoYCnVoRxlDoQ5DpYyhUIehUsZQqMMs88SAuZmZDS3zypiHmZkNIQ4eZmZW\nzMFjGJK0gKQtul2P+ZGkxZvIM0LS8Z2oT7c0sx/a/PzL95K2ejfq0k35s3VmJ8qeZwbMWyXpWmCu\nAZ6I+EhBGRcCvwUuj4i3C5//3t6ev1KP9xWUtTZwIrBiRGwg6X3AbhHx3UbyR8Tbkn4FfKDR56x7\n/i8PUP5PC8tbDVgrIq6WtBiwYES80mDeLYFjgdVIn2elKsQahXUYWSkDUiHXF+TfAjgZWBIYJen9\nwOcj4osD5Y2ItyRtVVLfXp5/TWBKRLwuaRvgfcDpEfFSQRmLA/8BjIqIQyStBawTEX8sKKPp/VAp\no7fP18vApIi4q8FiLpG0U0TMyGWuB4wHNiioxyeBKyLiFUnfBDYCvhsRdzSYv6XvaaWcrUjfj99J\n6gGWjIjHG8mbP1urSVo4X9qpbYbNgLmkjSsPFwU+AcyMiK8WlLEd8BlgM+A84HcR0d9Z7NW8q+XF\nw/P/M/L//QAiouErBUu6DvgK8OuI+EBOuy8iSr4YxwM3AxdG4YdA0jF5cR1gE2afc7MrcFtEfLqg\nrENIVwVYPiLWzD9YJ0XEtg3mfwj4d2AS8FYtPSJeKKjDD4F9gAcqZURE7FZQxq3AXsCEZt4TSSeS\nLrlzHvBqLT0iLmww/13AGGA0aVbixcD6EbFzwWs4l7QfD8g/dosDN0XEhgVltLQf8vZn5ddySU7a\nBbiH9NrOi4gfNVDGx4CvAh8jfU5PB/YrCD5Iuici3pd/vL8L/Bj4r4j4YIP52/E9PYa0L9aJiLUl\nrUzaB1sWlHE68B7S97T62So6yKs3bFoeETGpLulGSbcVlnE1cLWkZYB98/JTwG+A30fEm/3kfRJA\n0va1D1J2lKQ7gJLLzC8eEbdJqqbNLMgP8Hngy8BMSf/H7CP2pQfKGBHfBpB0PbBRrZUg6Vjg0sJ6\nHE668OWtuexHJL2zIP/LEXF54XPW24P05Xy9lUIi4qm69+StvrbtxaLAC0C1JRxAQ8EDeDufD7Un\n8IuI+IWkOwueH2DNiNhH0r4AEfFP1b2gRrS4HyBdQWKjiPgHzPoBvRT4ECm4DRg8IuJSSQsBVwJL\nAXtGxF8L61Gr98eAcbnMklZDO76ne5J6CO4AiIinJS1VWMaj+W8B0r5oi2ETPOr6QBcANgaWaaKc\ndwCfBvYH7gTOBLYCDgS2aawIbRkRN+YHW1A+9vR87qaIXMZewLSSAiKiHR+iFYFqU/iNnFbi9Yh4\no/YFk7SoMXR3AAAQEElEQVQg/XTv1UjaKC9eK+nHpB/ZWT/+jXYtZI8BC1XzN+Gp/F5G/tE6Aniw\n0cwR8ZkWnhvgzfyjfyCpBQjpNZV4I3cb1j5Xa1K+T1raD9k76573TVLXz2uS+q2PpF8w5+dnGdIP\n579KIiL+raAeUyX9Gtge+KGkRSj7rrb8PQXeiIiQVCtjicL8sw722m3YBA/SEUuQjrBnAo8DB5cU\nIOkiUhP4DGDXiKh9EM6VNLHBYg4GTsmtFwF/Bz5bUg/S0fo4YF1JU0mvpaGuIknrRsRDlR/fORT+\n6J4O3Jb3C6Qj+FML8gNcJ+nrwGKStge+yOzuiv78pO5x9Xo9wZxH8AP5J3CXpGuYMwCV/NAcBvw/\nUtfTVNIR7+H95qhoQ//4Z3IdjouIx/Pg8BkD5Kl3LHAFsGoeZN0SOKiwjJb2Q3YmcKuki/PjXYGz\n8g/nAwPkrf8e1vc4lNgb2BE4PiJekrQSqRuqUU1/TyvG5wC2bO7i/Sypp6NheZzkq8D6pBYuUDbe\n22u5w2XMox0kfTgirm1TWcsARMTLLZSxBLBAo4PLOc+4iDhUaQJBzawPQekHKgehrfPD6yOiqKtE\n0gKkgPpRUjD9E3Byo+MwktaIiMcGShugjAN7S4+I0wrKWD4iXqxLW73Rgc129I+3Q25Zb0Z6L26J\niKKruLa6Hyp5NgFqMwJvjIhGD87aRtKo3tIj4m+F5RR/T+vyb0/l+xERVxXmvxI4F/hPUnA/EJge\nEV9rpj6zyh0uwSM3ob9A6jcF+DPpi9rnOEUf5WxBGrirzso5vSD/MsAxlXpcB3ynJIhIWhH4HrBy\nROykNJNk84j4bUEZe5NmksyQ9C3STJL/LphJMgK4PyLWbfQ5+yjn48ClzY43SLojIjaqS5sUERv3\nlacTJN0IVGf3vIc0sNnogPntEbGJpDsrweOugQarJY2PiL3V+2y+iIj3F7yGS4CzSIPdrw60fR9l\ntLQfKuWMIHWBVr9nDf9oK028+D7pzqPVo+2GZ+FV9qlyGasDD0fE+g3mXxY4gLl/L0patC2rfR9q\nEwBy2u0RsUkr5Q6nbqsTSX3A/5Mf75/TPtdoAZLOANYE7qIyK4fUfdOoU4D7SE3iWj1+B3y8oIxT\nc55v5Md/JR1ZNBw8gG9GxPg8k+QjwPGk/dHQTJJIUwAfljSq9Eiszq7ACXnw/VxSQBtwUFHSuqRm\n+DI5ANUsTeXHohHt+KEhBfNLlGb5zJrdU5C/2f7xI/L/B5mzS0U0MLBc53jSrLMfSLqddMfOP0bE\n/xWU0ep+QNKXSAdYz5K+ZyLtl4ans5O+H8cAJwAfJnXrFY0tRsR76+q1EalbtVGXAbcA9wJFU/sr\nz/lx4IekcSBRMLGlonaAPC2/L08Dc50HUywihsUfcHcjaQOU8SC5tdZCPe5qJG2AMm7P/+9soYw7\n8//vA5+qL6/BMq4HXgGuIU0DnEA6ai3dJwsBu5H6up8kdVsNlGd30g/EC/l/7e/nwBaFz38DsC1p\nOuhqpL7/7zTxOvYAbiL9WKxdmHcN4GrS+MvUXKfVCvLf0UvaPU1+RkeQBonHAzMGcz/k/JOBdzRT\n90oZk/L/e+vTWiz33oJt53pPmtwX72mxjF1IEwc2AK4ljQPt1mrdhlPL4y1Ja0bEo5D6xSmfQngf\n8C7KZ0xUvSZpq4i4IddjS+C1wjJezX3TtaPUzUgnUZVodSYJwLcKt+9VRLwp6XLS61mM9OPTb4sw\nIi4GLpa0eUTc3GIVFouIayQp0pTqYyVNAv5roIxtnN0TEbFdtX9cDZwRLekLpKPhNSTdU1m1FHBj\ng89dLW8xUmtwH1JXZkPjPm2e5fQU5Z/neq/n8bRHlG7nMJV04mLDNOfJiguQ9sfTBUWckQe5/8ic\nEzFe7DvLXJ6NiNLZanOI2Sd5vkxqhbXFcAoeXyFN63yM1PRbjdSUHVDuCw7SF/IBpfNDqh+Ghk8m\nI427nFaZbfUiaQCrxJdJR/lr5j7mHtKJWSVanUlCRFyXx19qfae3RcRzJWVI2on0Q7UNaRzqZGZ3\n6TXiTkmHM/dMkpIZbK380LRrds8FpHMbqmMN55OmlPfnLOByUguyeq7QK4U/UkgaTzrn5grgl8B1\n0fiVFNo5y+kx4M+SLmXO71nJSW1HAIsD/wb8N+lHs/R7Vp3OPpN0rskFBfnfIJ1Y+A1mB9YgtTIb\nNVHp5M0/MOe+aPT8n7ad6T5XublZMyzko+t18sOHo8FBWkn/Qvqh/yFpytusVcAPo8EzTuvKXBog\n8sBiE/kXJL0WkV5L0cB/O+RB9x+TfvRFmnX1lYg4v6CMs0ljHZc3+n7U5T8PeAj4FPAdUv/6gxFx\nRL8Z5yxjE1KX5LKkH5qlgR9FxK2l9SlVGbv5EXMG76VJ+7Khwdk21WUH4OqIKG2Rt7sex/SWHk2c\nryBp8Yj4Z5P1+GREnDdQWj/5HwM2jcIZa3Vl/K6X5Cg5OOrUTL7hFjxanSnV28yeWTMYGiyj5dlW\nuZyWXks7SLob2L7W2lCaT351FMzwyfmabr3UZidp9qUkFgL+EhGbFZQxhnR0uBqzT6yLwve1qUF3\nSbuTuul2Y85bK78CnBMRNzVah2ZJ+khE/G/dxINZCo9yH6f3a8gVXWusVZI2J00gWTIimr3GVm/f\n97nS+sl/JbBHs8GrXZqdyTeQYdNt1cpMqTb3K7c826pNs77aYYG6H/oXKBw3Ubr43PHMbr38QlJJ\n66XW4npJ0gbAM6SZKSXOJB2ZNT0rhiZn97R57KZZ/wL8L2msozY1tfq/4eDBnCdrLgp8kgZn9kj6\nWUQcWekmnkNh9/DPgB3IATki7pb0of6zzKrHTsDOwEhJP6+sWpqyy4u8Sjr59FoKTz6V9NWI+FEv\nY0kNl1HRjjPd5zJsggfpQ71eNNfUalu/Mun6QZ+oPP620kXtSrTyWtrpCkl/As7Oj/eh/FbB3wQ2\nqW+9kPr7GzFO0nK5nAmksYrSgfzpETFh4M361fSgezZZ6Uz70czZmiy9+kCxiKh1E93H7KBBXn5Z\n0obR4AUFY+4LUv6sYD/Uzoi/Dri9bl3x5XSi+WtsPU0aw9mNOcduXiFdhLNRf8h/zagNkk+kgcv1\nDKC3M92Lpk/3ZjgFj6ZnSuUupZdJF0NsVTtmW7Vj1lfLIuIrkj5BuowFpIvHXdRfnl602no5g3SF\n5NHMnhlUen2tYySdTJpy3NSgJK3P7rkY+AspcHZrzGFj0oHJBFIAqV3N9jBJjV7Nttqls0Aur6Hf\nmZh98dJPkc73uS+XuS9wJGnWUqOavsZWRNwN3C3pzGjgnKN+ymn4CgW95K1doucBoP6gorSXYSqp\nZXwtqRU4gzR54DvN1g+GwZhH3UypDYFWZkq1oz7vJ73xtYsy/h04MCLu6TvXrLxD6rW0g9JFDd/H\nnK2Xe6LBSydIuoJ8rwfmvCR7/bWv+ivj98C6wP3M7rYqHZRsadC9HX3QrVI6UXPnmH012yVJM4x2\nJJ0jsV4DZVzL7CPlmcATpBl9DV/RVmka/fmkILI16SztXUrGBSWtQLrG1nakIPYn4IheWkb9ldHU\n+I3ae9b/w/TSpZpbt42WcQXwEunKvE19R3otdxgEj7bPlGqxPrW547Wj0n/Q4I1uhsprkfQKvTel\nmzn7lbrWy19KWi9tmTUiPRwR6wy8Zb9ltDTornSp75siorTbr22U7o3y3trMvTw78e6IWLc62NpH\n3trnujpeQl4unWZbm176B+BvpMupl7bOW6Z0LlXNrPGbiOi3C07SShExLU99nuus/4hoeCq6pBsi\notUbhXXkGmnzfbdVRFwHIGmh2nKN0glRg20Mc3YN7EeDXQND5bVEey7nXi3vAsrmz1fdJOm9EXFv\nC1W4SdJ6ETHQFVv70+qg+xHA0ZLeIE0CaCoQt6iVq9nWPhO1G4RdTHoNu5JayAPq5Uh9edLZ7rcq\nnWhYMvttDVLLY7Nc5s3Av0fBBTObHb+J2Vfbfnd9C0FpanaJdnSptuM7Mpfh0PKYNVOKdMZrzVKk\nq3WWXiK51fo03TUw1F5LK1ptvVR+aBYE1iKdWPZ6JX/JD82DpNlrj7dQRktHiHm8ZD9g9Yj4jtIV\nXVdqtNurXXILqtYKLL6abf58fyxm3yBsKdKFLwec6aTZd9vsVWFXzS3Ar5jdHToW+FJJ67yP8Zsv\nDNTt1M7vaStdqu38jvRa/jAIHssAy9GemVLtqE8rXQND6rV0U5t/aHotq7CMbUkTKpo6QlS6De3b\nwEci4j15BtmV0eKVTwdb7qN/X+QTPvPn+55WuwWbqMdc519JurtwvKF624LaPYB+EgPcerqd39NW\nulTb+R3pzXDotmrnTKl2aLprYAi+lq5p9YPfgbI+QzpCXIjKESKNnyPxwYjYSPnWsRHxd0kLt6Fe\ng60dNwhrh8slHUW6MnCQp5Er31F0oB/x3BI8KSLOLX3iNn9Pm+5Sbed3pDfzfctjKGq1a8CGnlYH\n3SXdSrr50e05iPSQWh59tkSHKrV4g7A21aF686naj9ysQfyBZkzlMiZGxJiBtuukdnSpdoqDh1kb\nKF2D6MfNDrpL2o85r2S7F+meKw1dR8nmpBZvdpbL+AHwPOnaa7MuWDmYXcTt6FLtFAcPszZo06D7\nuqT7igi4Jlq8FPdwptnXOtuKdN7N8cB/FQ6YD4nrdA1VDh5mbTCUjxCHI82+YOb3STdwOmugCSm9\nlLEYadbUVqQg8hfSOMign3MyFDl4mNl8R9IfSZfl2J7UZfUa6YrNJbOtxpMu5XFmTvoUsEzJSX7z\nMwcPM5vvSFqcdO7UvRHxiNLNzt4bEVcWlPFA/XlXvaUNV/P9VF0zG34i3UPjwsrjaZRfSPQOSZtF\nxC0Akj7I3HdMHLYcPMzMKipnZi9EOs/ib/nxaqS7VhrutjIzm0Onz8yeXzh4mJlZsaJbhpqZmYGD\nh5mZNcHBw6wFkk6QdGTl8Z/y/Rdqj39SuVFSadnHSvrPdtTTrN0cPMxacyPpgoa1K7GuAKxfWb8F\ncNNAhUjyzEebpzh4mLXmJmDzvLw+cB/wiqTl8r0s3gPcKenHku6TdK+kfQAkbSPpL5ImkC/HL+kb\nkv4q6QbSXfnMhiQf7Zi1ICKeljQz3/lvC9LtTkeSAsrLpNvS7gJsCLyf1DK5Pd9xD9KlMzaIiMcl\nbUy6492GpO/mHcCkwXw9Zo1y8DBr3U2kwLEF8FNS8NiCFDxuJF1Y7+yIeAt4VtJ1pPt8zyBdb6l2\n74mtgYvy2dHkFonZkORuK7PW1cY93kvqtrqF1PJoZLzj1QHWmw1JDh5mrbuJ1DX1YkS8lW8WtCwp\ngNxEupT3PpJG5DsEfgi4rZdyrgf2kLSYpKVItyg2G5LcbWXWuntJYxln1aUtGRHP5/t5bw7cTbpG\n0lcj4pl886dZIuIOSefm7Z4Dbh+U2ps1wZcnMTOzYu62MjOzYg4eZmZWzMHDzMyKOXiYmVkxBw8z\nMyvm4GFmZsUcPMzMrJiDh5mZFfv/pdXfHkLf6AwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a18bcd5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = sns.barplot(data=df,x=0, y=1)\n",
    "ax.set_title('Word Frequencies  Shakespeare All Text')\n",
    "ax.set(xlabel='Word', ylabel='Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional topic 1: DataFrames\n",
    "\n",
    "Pandas and Spark dataframes can be easily converted to each other, making it easier to work with different data formats. This section shows some examples of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>73.847017</td>\n",
       "      <td>241.893563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>68.781904</td>\n",
       "      <td>162.310473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>74.110105</td>\n",
       "      <td>212.740856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>71.730978</td>\n",
       "      <td>220.042470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>69.881796</td>\n",
       "      <td>206.349801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender     Height      Weight\n",
       "0   Male  73.847017  241.893563\n",
       "1   Male  68.781904  162.310473\n",
       "2   Male  74.110105  212.740856\n",
       "3   Male  71.730978  220.042470\n",
       "4   Male  69.881796  206.349801"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"sparklect/01_heights_weights_genders.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this pandas dataframe to a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Gender: string, Height: double, Weight: double]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)\n",
    "sparkdf = sqlsc.createDataFrame(df)\n",
    "sparkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+\n",
      "|Gender|           Height|          Weight|\n",
      "+------+-----------------+----------------+\n",
      "|  Male|  73.847017017515|241.893563180437|\n",
      "|  Male|68.78190404589029|  162.3104725213|\n",
      "|  Male|74.11010539178491|  212.7408555565|\n",
      "|  Male| 71.7309784033377|220.042470303077|\n",
      "|  Male| 69.8817958611153|206.349800623871|\n",
      "+------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparkdf.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can't call .map() on a DataFrame directly - you first have to convert it into an RDD\n",
    "temp = sparkdf.rdd.map(lambda r: r.Gender)\n",
    "print(type(temp))\n",
    "temp.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional topic 2: Machine Learning using Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a data set from the Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [73.8470170175,241.89356318]),\n",
       " LabeledPoint(1.0, [68.7819040459,162.310472521]),\n",
       " LabeledPoint(1.0, [74.1101053918,212.740855557]),\n",
       " LabeledPoint(1.0, [71.7309784033,220.042470303]),\n",
       " LabeledPoint(1.0, [69.8817958611,206.349800624])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=sparkdf.rdd.map(lambda row: LabeledPoint(row.Gender=='Male',[row.Height, row.Weight]))\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, DenseVector([73.847, 241.8936]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=sparkdf.rdd.map(lambda row: LabeledPoint(row[0]=='Male',row[1:]))\n",
    "data2.take(1)[0].label, data2.take(1)[0].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[157] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.7,0.3])\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the logistic regression model using MLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.4768, 0.1961])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 147.0 failed 1 times, most recent failure: Lost task 0.0 in stage 147.0 (TID 404, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-140-c93c79069485>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/classification.py\", line 206, in predict\n    margin = self.weights.dot(x) + self._intercept\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 372, in dot\n    assert len(self) == _vector_size(other), \"dimension mismatch\"\nAssertionError: dimension mismatch\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor193.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-140-c93c79069485>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/classification.py\", line 206, in predict\n    margin = self.weights.dot(x) + self._intercept\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 372, in dot\n    assert len(self) == _vector_size(other), \"dimension mismatch\"\nAssertionError: dimension mismatch\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-c93c79069485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 147.0 failed 1 times, most recent failure: Lost task 0.0 in stage 147.0 (TID 404, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-140-c93c79069485>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/classification.py\", line 206, in predict\n    margin = self.weights.dot(x) + self._intercept\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 372, in dot\n    assert len(self) == _vector_size(other), \"dimension mismatch\"\nAssertionError: dimension mismatch\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor193.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-140-c93c79069485>\", line 1, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/classification.py\", line 206, in predict\n    margin = self.weights.dot(x) + self._intercept\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 372, in dot\n    assert len(self) == _vector_size(other), \"dimension mismatch\"\nAssertionError: dimension mismatch\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "results = test.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "print(results.take(10))\n",
    "type(results)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure accuracy and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy=results.filter(lambda a,p: a==p).count()/float(results.count())\n",
    "#test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.evaluation.BinaryClassificationMetrics'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9278202755163886"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(metrics))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf mylogistic.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(sc, \"mylogistic.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline API automates a lot of this stuff, allowing us to work directly on dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdd.saveAsTextFile()` saves an RDD as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional Topic 3: Your Turn at Machine Learning! :)\n",
    "\n",
    "For this exercise, we're going to use one of the datasets we've already worked with: the Boston House Prices dataset. We're going to try a couple of regression algorithms, but from the SparkML library this time.\n",
    "\n",
    "Before you proceed, make sure to do an overview of the documentation: \n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load the dataset, which resides as a CSV file in the folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio   black  lstat  medv\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3  396.90   4.98  24.0\n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8  396.90   9.14  21.6\n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8  392.83   4.03  34.7\n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7  394.63   2.94  33.4\n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7  396.90   5.33  36.2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path: /sparklect/boston.csv\n",
    "bdf=pd.read_csv(\"sparklect/boston.csv\")\n",
    "bdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data to make sure everything is loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[crim: double, zn: double, indus: double, chas: bigint, nox: double, rm: double, age: double, dis: double, rad: bigint, tax: bigint, ptratio: double, black: double, lstat: double, medv: double]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sqlsc=SQLContext(sc)\n",
    "bsparkdf = sqlsc.createDataFrame(bdf)\n",
    "bsparkdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to create a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+----+-------------------+------------------+-----+------+---+---+-------+------+-----+----+\n",
      "|                crim|  zn|indus|chas|                nox|                rm|  age|   dis|rad|tax|ptratio| black|lstat|medv|\n",
      "+--------------------+----+-----+----+-------------------+------------------+-----+------+---+---+-------+------+-----+----+\n",
      "|             0.00632|18.0| 2.31|   0| 0.5379999999999999|             6.575| 65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|             0.02731| 0.0| 7.07|   0|              0.469|             6.421| 78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|             0.02729| 0.0| 7.07|   0|              0.469|             7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
      "|0.032369999999999996| 0.0| 2.18|   0|0.45799999999999996| 6.997999999999999| 45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|             0.06905| 0.0| 2.18|   0|0.45799999999999996|             7.147| 54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "|             0.02985| 0.0| 2.18|   0|0.45799999999999996|              6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|28.7|\n",
      "|             0.08829|12.5| 7.87|   0|              0.524|6.0120000000000005| 66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|\n",
      "|             0.14455|12.5| 7.87|   0|              0.524| 6.172000000000001| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|27.1|\n",
      "|             0.21124|12.5| 7.87|   0|              0.524|             5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|16.5|\n",
      "|             0.17004|12.5| 7.87|   0|              0.524|             6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|18.9|\n",
      "+--------------------+----+-----+----+-------------------+------------------+-----+------+---+---+-------+------+-----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bsparkdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll first have to vectorize the features\n",
    "pricing = bsparkdf.rdd.map(lambda r: r.medv)\n",
    "print(type(pricing))\n",
    "pricing.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(24.0, [0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,4.98]),\n",
       " LabeledPoint(21.6, [0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,9.14]),\n",
       " LabeledPoint(34.7, [0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671,2.0,242.0,17.8,4.03]),\n",
       " LabeledPoint(33.4, [0.03237,0.0,2.18,0.0,0.458,6.998,45.8,6.0622,3.0,222.0,18.7,2.94]),\n",
       " LabeledPoint(36.2, [0.06905,0.0,2.18,0.0,0.458,7.147,54.2,6.0622,3.0,222.0,18.7,5.33])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data=bsparkdf.rdd.map(lambda r: LabeledPoint(r.medv,[r.crim, r.zn,r.indus,r.chas,r.nox,r.rm,r.age,r.dis,r.rad,r.tax,r.ptratio,r.lstat]))\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, fit a Linear Regression model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[252] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.7,0.3])\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrm = LinearRegressionWithSGD.train(sc.parallelize(data), iterations=10\n",
    "...     initialWeights=np.array([1.0]))\n",
    ">>> abs(lrm.predict(np.array([0.0])) - 0) < 0.5\n",
    "True\n",
    ">>> abs(lrm.predict(np.array([1.0])) - 1) < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "lrm = LinearRegressionWithSGD.train(train, iterations=10, step=0.1)\n",
    "#lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now validate the model on the test set, and check the Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(34.7, -5.58410498067677e+40), (21.7, -7.045037433418534e+40), (13.6, -7.1630638256302345e+40), (15.6, -7.146288192713833e+40), (13.9, -7.117460565796236e+40), (16.6, -7.13223368388286e+40), (18.4, -7.145222520405285e+40), (13.2, -7.1121956497814e+40), (20.0, -6.409218682795631e+40), (24.7, -5.200035999096695e+40)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = test.map(lambda lp: (lp.label, float(lrm.predict(lp.features))))\n",
    "print(results.take(10))\n",
    "type(results)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 1.0251912952906833e+82\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SE = results.map(lambda vp: (vp[0] - vp[1])**2).reduce(lambda x, y: x + y)\n",
    "print(\"Mean Squared Error = \" + str(MSE/results.count()))\n",
    "#print(results.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Linear Regression with a more powerful algorithm - the Random Forest. As the Random Forest has several hyperparameters that can be tuned for maximum accuracy, we're going to need to use k-fold Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up a grid for the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 1.02519129529e+82\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lrm.predict(test.map(lambda x: x.features))\n",
    "testresults = test.map(lambda lp: lp.label).zip(pred_lr)\n",
    "testMSE = testresults.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(test.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with a Random Forest regressor using k-fold Cross Validation, and find the optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = RandomForest.trainRegressor(train,categoricalFeaturesInfo={}, numTrees=3, maxDepth=4, maxBins=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, validate the model on the test set and check the Root Mean Squared Error again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 17.890009790547364\n",
      "Learned regression forest model:\n",
      "TreeEnsembleModel regressor with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 9 <= 264.5)\n",
      "     If (feature 5 <= 6.5115)\n",
      "      If (feature 10 <= 18.95)\n",
      "       If (feature 11 <= 7.41)\n",
      "        Predict: 27.924999999999997\n",
      "       Else (feature 11 > 7.41)\n",
      "        Predict: 21.34444444444444\n",
      "      Else (feature 10 > 18.95)\n",
      "       If (feature 0 <= 0.101185)\n",
      "        Predict: 19.740000000000002\n",
      "       Else (feature 0 > 0.101185)\n",
      "        Predict: 16.499999999999993\n",
      "     Else (feature 5 > 6.5115)\n",
      "      If (feature 5 <= 7.3725000000000005)\n",
      "       If (feature 11 <= 4.475)\n",
      "        Predict: 31.525\n",
      "       Else (feature 11 > 4.475)\n",
      "        Predict: 34.48695652173913\n",
      "      Else (feature 5 > 7.3725000000000005)\n",
      "       If (feature 11 <= 4.475)\n",
      "        Predict: 46.9\n",
      "       Else (feature 11 > 4.475)\n",
      "        Predict: 35.19999999999999\n",
      "    Else (feature 9 > 264.5)\n",
      "     If (feature 5 <= 7.005999999999999)\n",
      "      If (feature 11 <= 14.115)\n",
      "       If (feature 2 <= 16.57)\n",
      "        Predict: 22.696000000000005\n",
      "       Else (feature 2 > 16.57)\n",
      "        Predict: 26.27407407407408\n",
      "      Else (feature 11 > 14.115)\n",
      "       If (feature 11 <= 19.259999999999998)\n",
      "        Predict: 16.398360655737704\n",
      "       Else (feature 11 > 19.259999999999998)\n",
      "        Predict: 11.627906976744185\n",
      "     Else (feature 5 > 7.005999999999999)\n",
      "      If (feature 4 <= 0.5349999999999999)\n",
      "       If (feature 11 <= 3.715)\n",
      "        Predict: 40.2\n",
      "       Else (feature 11 > 3.715)\n",
      "        Predict: 32.0\n",
      "      Else (feature 4 > 0.5349999999999999)\n",
      "       Predict: 50.0\n",
      "  Tree 1:\n",
      "    If (feature 5 <= 6.8765)\n",
      "     If (feature 11 <= 14.68)\n",
      "      If (feature 5 <= 6.5115)\n",
      "       If (feature 7 <= 1.3513000000000002)\n",
      "        Predict: 50.0\n",
      "       Else (feature 7 > 1.3513000000000002)\n",
      "        Predict: 21.814492753623185\n",
      "      Else (feature 5 > 6.5115)\n",
      "       If (feature 4 <= 0.5125)\n",
      "        Predict: 29.27567567567568\n",
      "       Else (feature 4 > 0.5125)\n",
      "        Predict: 22.44999999999999\n",
      "     Else (feature 11 > 14.68)\n",
      "      If (feature 4 <= 0.6275)\n",
      "       If (feature 11 <= 18.1)\n",
      "        Predict: 18.406451612903226\n",
      "       Else (feature 11 > 18.1)\n",
      "        Predict: 16.152631578947368\n",
      "      Else (feature 4 > 0.6275)\n",
      "       If (feature 0 <= 11.8817)\n",
      "        Predict: 14.107692307692309\n",
      "       Else (feature 0 > 11.8817)\n",
      "        Predict: 8.619047619047613\n",
      "    Else (feature 5 > 6.8765)\n",
      "     If (feature 10 <= 14.75)\n",
      "      If (feature 11 <= 6.48)\n",
      "       If (feature 6 <= 97.05)\n",
      "        Predict: 48.59166666666667\n",
      "       Else (feature 6 > 97.05)\n",
      "        Predict: 41.30000000000001\n",
      "      Else (feature 11 > 6.48)\n",
      "       Predict: 30.900000000000016\n",
      "     Else (feature 10 > 14.75)\n",
      "      If (feature 11 <= 4.475)\n",
      "       If (feature 3 <= 0.5)\n",
      "        Predict: 38.37692307692308\n",
      "       Else (feature 3 > 0.5)\n",
      "        Predict: 48.666666666666664\n",
      "      Else (feature 11 > 4.475)\n",
      "       If (feature 0 <= 0.101185)\n",
      "        Predict: 33.49473684210526\n",
      "       Else (feature 0 > 0.101185)\n",
      "        Predict: 24.71\n",
      "  Tree 2:\n",
      "    If (feature 11 <= 9.870000000000001)\n",
      "     If (feature 6 <= 89.35)\n",
      "      If (feature 5 <= 7.1665)\n",
      "       If (feature 5 <= 6.797000000000001)\n",
      "        Predict: 24.263529411764708\n",
      "       Else (feature 5 > 6.797000000000001)\n",
      "        Predict: 30.535483870967752\n",
      "      Else (feature 5 > 7.1665)\n",
      "       If (feature 11 <= 3.715)\n",
      "        Predict: 44.02727272727273\n",
      "       Else (feature 11 > 3.715)\n",
      "        Predict: 34.82352941176471\n",
      "     Else (feature 6 > 89.35)\n",
      "      If (feature 5 <= 7.3725000000000005)\n",
      "       If (feature 10 <= 17.85)\n",
      "        Predict: 30.391666666666666\n",
      "       Else (feature 10 > 17.85)\n",
      "        Predict: 50.00000000000001\n",
      "      Else (feature 5 > 7.3725000000000005)\n",
      "       If (feature 0 <= 0.54251)\n",
      "        Predict: 48.8\n",
      "       Else (feature 0 > 0.54251)\n",
      "        Predict: 50.00000000000001\n",
      "    Else (feature 11 > 9.870000000000001)\n",
      "     If (feature 11 <= 15.53)\n",
      "      If (feature 9 <= 280.5)\n",
      "       If (feature 4 <= 0.582)\n",
      "        Predict: 22.586666666666666\n",
      "       Else (feature 4 > 0.582)\n",
      "        Predict: 30.80000000000001\n",
      "      Else (feature 9 > 280.5)\n",
      "       If (feature 5 <= 4.9715)\n",
      "        Predict: 23.100000000000005\n",
      "       Else (feature 5 > 4.9715)\n",
      "        Predict: 18.956521739130434\n",
      "     Else (feature 11 > 15.53)\n",
      "      If (feature 0 <= 5.82258)\n",
      "       If (feature 11 <= 17.36)\n",
      "        Predict: 18.44\n",
      "       Else (feature 11 > 17.36)\n",
      "        Predict: 15.054054054054054\n",
      "      Else (feature 0 > 5.82258)\n",
      "       Predict: 12.00757575757576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "pred = rfm.predict(test.map(lambda x: x.features))\n",
    "labelsAndPredictions = test.map(lambda lp: lp.label).zip(pred)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(test.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression forest model:')\n",
    "print(rfm.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
